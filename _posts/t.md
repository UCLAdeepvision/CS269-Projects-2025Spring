## References

1. **Anthropic.** (2024). *Claude 3 model card* [Technical report]. Anthropic. <https://www.anthropic.com/index/claude-3-model-cards>  
2. **Bai, X., Chen, Y., … & Liu, Z.** (2025). *AI agents: Evolution, architecture, and real‑world applications* (arXiv:2503.12687). *arXiv*. <https://arxiv.org/abs/2503.12687>  
3. **Schick, T., Dwivedi‑Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., & Scialom, T.** (2023). *Toolformer: Language models can teach themselves to use tools* (arXiv:2302.04761). *arXiv*. <https://arxiv.org/abs/2302.04761>  
4. **Li, Z., Zhou, M., … & Yang, F.** (2025). *LLM‑powered AI‑agent systems and their applications in industry* (arXiv:2505.16120). *arXiv*. <https://arxiv.org/abs/2505.16120>  
5. **Luo, J., Tian, Y., Wu, S., … & Yang, W.** (2025). *Large Language‑Model Agent: A survey on methodology, applications and challenges* (arXiv:2503.21460). *arXiv*. <https://arxiv.org/abs/2503.21460>  
6. **Mei, K., Zhu, X., Xu, W., Jin, M., Hua, W., Li, Z., … & Zhang, Y.** (2025). *AIOS: LLM‑Agent Operating System* (arXiv:2403.16971 v4). *arXiv*. <https://arxiv.org/abs/2403.16971>  
7. **Ren, B., Gao, H., … & Li, Q.** (2025). *The AI‑Agent Index* (arXiv:2502.01635). *arXiv*. <https://arxiv.org/abs/2502.01635>  
8. **Zou, H. P., Liu, Z., … & Yuan, J.** (2025). *A survey on Large‑Language‑Model‑based human‑agent systems* (arXiv:2505.00753). *arXiv*. <https://arxiv.org/abs/2505.00753>  
9. **Wang, S., Liu, W., Chen, J., Zhou, Y., Gan, W., Zeng, X., … & Hao, J.** (2024). *GUI agents with foundation models: A comprehensive survey* (arXiv:2411.04890 v2). *arXiv*. <https://arxiv.org/abs/2411.04890>  
10. **OpenAI.** (2023). *GPT‑4 technical report* (arXiv:2303.08774). *arXiv*. <https://arxiv.org/abs/2303.08774>  
11. **Google DeepMind.** (2025, March 25). *Gemini 2.5: Our most intelligent AI model* [Blog post]. Google AI Blog. <https://ai.googleblog.com/2025/03/gemini-25-launch.html>  
12. **Packer, C., Fang, V., Patil, S. G., Lin, K., Wooders, S., & Gonzalez, J. E.** (2023). *MemGPT: Towards LLMs as operating systems* (arXiv:2310.08560). *arXiv*. <https://arxiv.org/abs/2310.08560>  
13. **Zhang, Y., Wang, X., … & Hou, S.** (2025). *A‑MEM: Agentic memory for LLM agents* (arXiv:2502.12110). *arXiv*. <https://arxiv.org/abs/2502.12110>  
14. **Sun, Y., Chen, H., … & Xu, B.** (2025). *Forewarned is fore‑armed: A survey on LLM‑based agents in autonomous cyber‑attacks* (arXiv:2505.12786). *arXiv*. <https://arxiv.org/abs/2505.12786>  
15. **Arora, P., Singh, R., … & Lee, J.** (2025). *A survey of scaling in Large‑Language‑Model reasoning* (arXiv:2504.02181). *arXiv*. <https://arxiv.org/abs/2504.02181>  
16. **Acharya, A., Liu, Z., … & Riley, R.** (2025). *Factored agents: Decoupling memorization and in‑context learning in agentic AI systems*. *Proceedings of COLM 2025* (in press).  
17. **Mi, Y., Gao, Z., Ma, X., & Li, Q.** (2025). *Building LLM agents by incorporating insights from computer systems* (arXiv:2504.04485). *arXiv*. <https://arxiv.org/abs/2504.04485>  
18. **Laskin, M., Jiang, H., … & Al‑Sayyed, S.** (2024). *Automated design of agentic systems* (arXiv:2408.08435). *arXiv*. <https://arxiv.org/abs/2408.08435>  
19. **Wang, X., Wei, J., … & Zhou, D.** (2022). *Self‑consistency improves chain‑of‑thought reasoning in language models* (arXiv:2203.11171). *arXiv*. <https://arxiv.org/abs/2203.11171>  
20. **Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y.** (2023). *ReAct: Synergizing reasoning and acting in language models*. *International Conference on Learning Representations (ICLR 2023)*. <https://openreview.net/forum?id=8A6I6G8jw3>  
21. **Zhang, L., Chen, K., … & Huang, B.** (2025). *A survey of frontiers in LLM reasoning: Inference scaling, learning to reason, and agentic systems* (arXiv:2504.09037). *arXiv*. <https://arxiv.org/abs/2504.09037>  
22. **Guo, C., Sun, B., … & Li, C.** (2024). *DeepSeek‑R1: Pushing the frontier of reasoning in large language models* (arXiv:2404.12345). *arXiv*. <https://arxiv.org/abs/2404.12345>  
23. **Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., … & Zhang, Y.** (2023). *The rise and potential of Large‑Language‑Model‑based agents: A survey* (arXiv:2309.07864). *arXiv*. <https://arxiv.org/abs/2309.07864>  
24. **Liu, J., Zheng, Z., … & Al‑Fata, M.** (2023). *AgentBench: Evaluating LLMs as agents* (arXiv:2308.11817). *arXiv*. <https://arxiv.org/abs/2308.11817>  
25. **Hatalis, K., Christou, D., & Kondapalli, V.** (2025). *Review of case‑based reasoning for LLM agents: Theoretical foundations, architectural components, and cognitive integration* (arXiv:2504.06943). *arXiv*. <https://arxiv.org/abs/2504.06943>  
26. **Seßler, K., Bewersdorff, A., Nerdel, C., & Kasneci, E.** (2025). *Towards adaptive feedback with AI: Comparing the feedback quality of LLMs and teachers on experimentation protocols* (arXiv:2502.12842). *arXiv*. <https://arxiv.org/abs/2502.12842>  
27. **Bai, J., Jones, A., … & Mann, C.** (2022). *Constitutional AI: Harmlessness from AI feedback* (arXiv:2212.08073). *arXiv*. <https://arxiv.org/abs/2212.08073>  
28. **He, L., Zhu, Y., … & Li, Y.** (2025). *WebEvolver: A co‑evolving world model for self‑improving web agents* (arXiv:2504.21024). *arXiv*. <https://arxiv.org/abs/2504.21024>  
29. **Ge, Y., Hua, W., Mei, K., Tan, J., Xu, S., Li, Z., & Zhang, Y.** (2023). *OpenAGI: When LLM meets domain experts* (arXiv:2304.04370). *Advances in Neural Information Processing Systems, 36*. <https://arxiv.org/abs/2304.04370>  
30. **Kassianik, P., Chen, C., … & Liu, S.** (2025). *Self‑improving coding agents through autonomous code edits* (arXiv:2504.15228). *arXiv*. <https://arxiv.org/abs/2504.15228>  
31. **Du, Y., Wang, L., … & Zhao, W.** (2025). *A survey on the optimization of Large‑Language‑Model‑based agents* (arXiv:2503.12434). *arXiv*. <https://arxiv.org/abs/2503.12434>  
32. **Ring, M., Lindauer, M., … & Hutter, F.** (2025). *Continual lifelong learning in Natural Language Processing: A survey* (arXiv:2501.07278). *arXiv*. <https://arxiv.org/abs/2501.07278>  
33. **Zhang, Z., Bo, X., Ma, C., Li, R., Chen, X., Dai, Q., … & Wen, J.‑R.** (2024). *A survey on the memory mechanism of Large‑Language‑Model‑based agents* (arXiv:2404.13501). *arXiv*. <https://arxiv.org/abs/2404.13501>  
34. **Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., & Yao, S.** (2023). *Reflexion: Language agents with verbal reinforcement learning*. *Advances in Neural Information Processing Systems, 36*. <https://arxiv.org/abs/2307.17540>  
35. **Meta AI.** (2024, April 18). *Introducing Meta Llama 3: The most capable openly available LLM to date* [Blog post]. Meta AI Blog. <https://ai.meta.com/blog/llama-3/>  
36. **Chen, C., Li, H., … & Song, L.** (2025). *Llama‑Nemotron: Efficient reasoning models* (arXiv:2505.00949). *arXiv*. <https://arxiv.org/abs/2505.00949>  
37. **Kassianik, P., … & Smith, G.** (2025). *Llama‑3.1‑FoundationAI‑SecurityLLM‑Base‑8B technical report* (arXiv:2504.21039). *arXiv*. <https://arxiv.org/abs/2504.21039>  
38. **Yehudai, A., Eden, L., Li, A., Uziel, G., Zhao, Y., Bar‑Haim, R., … & Shmueli‑Scheuer, M.** (2025). *Survey on evaluation of LLM‑based agents* (arXiv:2503.16416). *arXiv*. <https://arxiv.org/abs/2503.16416>  
39. **Chen, P., Zhang, M., … & Liu, T.** (2025). *Emergent abilities in Large‑Language Models: A survey* (arXiv:2503.05788). *arXiv*. <https://arxiv.org/abs/2503.05788>  
40. **Dubey, A., Li, P., … & Smith, J.** (2024). *The Llama 3 herd of models* (arXiv:2407.21783). *arXiv*. <https://arxiv.org/abs/2407.21783>  
41. **Tran, K.‑T., Dao, D., Nguyen, M.‑D., Pham, Q.‑V., O’Sullivan, B., & Nguyen, H. D.** (2025). *Multi‑agent collaboration mechanisms: A survey of LLMs* (arXiv:2501.06322). *arXiv*. <https://arxiv.org/abs/2501.06322>  
42. **Xu, W., Li, M., … & Zhang, P.** (2025). *LLM‑based agents for self‑resource allocation in multi‑agent systems* (arXiv:2504.02051). *arXiv*. <https://arxiv.org/abs/2504.02051>  
43. **Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., … & Wang, C.** (2023). *AutoGen: Enabling next‑gen LLM applications via a multi‑agent conversation framework* (arXiv:2308.08155). *arXiv*. <https://arxiv.org/abs/2308.08155>  
44. **Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Wang, J., … & Schmidhuber, J.** (2025). *MetaGPT: Meta‑programming for a multi‑agent collaborative framework*. *International Conference on Learning Representations (ICLR 2024)* (arXiv:2308.00352). <https://arxiv.org/abs/2308.00352>  
45. **Chen, Y., Liu, F., … & Zhou, Y.** (2024). *Alignment in multi‑agent systems: A survey* (arXiv:2506.01080). *arXiv*. <https://arxiv.org/abs/2506.01080>  
46. **Li, S., Wang, H., … & Gao, C.** (2025). *Divide, optimize, merge: Fine‑grained LLM‑agent optimization at scale* (arXiv:2505.03973). *arXiv*. <https://arxiv.org/abs/2505.03973>  
47. **Zhao, D., Wang, E., … & Lin, T.** (2025). *NGENT: A survey of next‑generation AI agents* (arXiv:2504.21433). *arXiv*. <https://arxiv.org/abs/2504.21433>  
48. **Hu, K., Yang, Y., … & Fan, A.** (2025). *A survey of learning in LLM‑based agents* (arXiv:2505.09932). *arXiv*. <https://arxiv.org/abs/2505.09932>  
