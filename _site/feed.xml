<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/CS269-Projects-2025Spring/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/CS269-Projects-2025Spring/" rel="alternate" type="text/html" /><updated>2025-06-07T23:25:41-07:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/feed.xml</id><title type="html">2025S, UCLA CS269 Course Projects</title><subtitle>Course projects for UCLA CS269, Seminar on AI Agents and Foundation Models</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Recent Developments in GUI Web Agents</title><link href="http://localhost:4000/CS269-Projects-2025Spring/2025/06/03/student-27-WebAgent.html" rel="alternate" type="text/html" title="Recent Developments in GUI Web Agents" /><published>2025-06-03T00:00:00-07:00</published><updated>2025-06-03T00:00:00-07:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/2025/06/03/student-27-WebAgent</id><content type="html" xml:base="http://localhost:4000/CS269-Projects-2025Spring/2025/06/03/student-27-WebAgent.html"><![CDATA[<blockquote>
  <p>Web agents are a new class of agents that can interact with the web. They are able to navigate the web, search for information, and perform tasks. They are a type of multi-modal agent that can use text, images, and other modalities to interact with the web. Since 2024, we have seen a surge in the development of web agents, with many new agents being developed and released. In this blog post, we survey the recent developments in the field of web and particularly GUI agents, and provide a comprehensive overview of the state of the art. We review core benchmarks - WebArena, VisualWebArena, Mind2Web, and AssistantBench — that have enabled systematic measurement of these capabilities. We discuss the backbone vision-language models that power these agents, as well as the recent advancement in reasoning.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#background-from-text-based-to-multi-modal-web-agents" id="markdown-toc-background-from-text-based-to-multi-modal-web-agents">Background: From Text-Based to Multi-modal Web Agents</a></li>
  <li><a href="#core-benchmarks" id="markdown-toc-core-benchmarks">Core Benchmarks</a>    <ul>
      <li><a href="#visualwebarena" id="markdown-toc-visualwebarena">VisualWebArena</a></li>
      <li><a href="#mind2web" id="markdown-toc-mind2web">Mind2Web</a></li>
      <li><a href="#online-mind2web" id="markdown-toc-online-mind2web">Online-Mind2Web</a></li>
      <li><a href="#webvoyager" id="markdown-toc-webvoyager">WebVoyager</a></li>
      <li><a href="#realwebassist" id="markdown-toc-realwebassist">RealWebAssist</a></li>
    </ul>
  </li>
  <li><a href="#backbone-vision-language-models-and-ui-grounding" id="markdown-toc-backbone-vision-language-models-and-ui-grounding">Backbone Vision-Language Models and UI Grounding</a>    <ul>
      <li><a href="#ui-grounding---aligning-perception-and-action" id="markdown-toc-ui-grounding---aligning-perception-and-action">UI Grounding - Aligning Perception and Action</a></li>
    </ul>
  </li>
  <li><a href="#architectures-and-methods-enhancing-reasoning-in-gui-agents" id="markdown-toc-architectures-and-methods-enhancing-reasoning-in-gui-agents">Architectures and Methods: Enhancing Reasoning in GUI Agents</a>    <ul>
      <li><a href="#prompt-based-planning-and-chain-of-thought" id="markdown-toc-prompt-based-planning-and-chain-of-thought">Prompt-Based Planning and Chain-of-Thought</a></li>
      <li><a href="#reinforcement-learning" id="markdown-toc-reinforcement-learning">Reinforcement Learning</a></li>
    </ul>
  </li>
  <li><a href="#replication-study-evaluating-qwen-25-vl-as-a-gui-agent" id="markdown-toc-replication-study-evaluating-qwen-25-vl-as-a-gui-agent">Replication Study: Evaluating Qwen-2.5-VL as a GUI Agent</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Graphical User Interface (GUI) web agents are autonomous agents that interact with websites much like a human user – by clicking, typing, and reading on web pages. In recent years, especially since 2023/2024, there has been rapid progress in developing general-purpose GUI web agents powered by large (multimodal) language models (LLMs). These agents aim to follow high-level instructions (e.g. “Find the cheapest red jacket on an online store and add it to the cart”) and execute various tasks across different websites.</p>

<p>A key challenge driving current research is reasoning capability: GUI agents must plan multi-step actions, handle dynamic web content, hop between different web pages, and sometimes recover from mistakes or unexpected outcomes. This report surveys recent developments in this area, emphasizing how new architectures and training methods have enhanced reasoning, with a special focus on e-commerce applications (a domain that is naitively multi-modal, and demands sophisticated multi-step reasoning, such as searching for products, comparing options, and completing purchases). We also contrast these GUI agents with earlier text-based web agents to highlight differences in reasoning approaches.</p>

<h2 id="background-from-text-based-to-multi-modal-web-agents">Background: From Text-Based to Multi-modal Web Agents</h2>

<p>Early “web agents” were often text-based, interfacing with the web via textual inputs/outputs or APIs rather than through a visual GUI. For example, a text-based agent might read the HTML or use a browser’s accessibility layer to get page text, then choose an action like following a link or submitting a form by ID. Notable early systems included OpenAI’s WebGPT (2021) [1] for web question-answering and various reinforcement learning (RL) agents on simplified web environments. These text-based agents relied on parsing textual content and often used search engine APIs or DOM trees. Their reasoning largely resembled traditional NLP tasks – e.g. retrieving relevant information or doing reading comprehension – and they did not need to reason about layout or visual elements.</p>

<p>By contrast, GUI web agents operate on the actual rendered web interface (like a human using a browser). This introduces new challenges and differences in reasoning.</p>

<p>The first one is <em>Perception and Grounding</em>: GUI agents must interpret the visual layout and GUI elements. They may receive a DOM tree or a rendered screenshot of the page. For instance, a task might involve recognizing a “red BUY button” or an image of a product. Text-only models struggle with such visual cues. Thus, GUI agents often incorporate vision (to understand images, colors, iconography) in addition to text. Reasoning for GUI agents often means linking instructions to the correct GUI element or image – a form of grounded reasoning not present in purely text agents.</p>

<p>The second one is <em>Action Space &amp; Planning</em>: A text-based agent’s actions can be abstract (e.g. “go to URL” or “click link by name”), whereas a GUI agent deals with low-level events (mouse clicks, typing) on specific coordinates or element IDs. This requires more procedural reasoning: the agent must plan a sequence of GUI operations to achieve the goal. Often the agent must navigate through multiple pages or UI states, which demands long-horizon planning. For example, buying a product involves searching, filtering results, clicking the item, adding to cart, and possibly checkout – multiple steps that must be reasoned about and executed in order. Modern GUI agents leverage LLMs’ ability to plan by breaking down goals into sub-goals, sometimes explicitly writing out multi-step plans [2]. In comparison, text-based agents typically handle shorter-horizon tasks (like finding a specific fact on Wikipedia) with simpler sequential reasoning.</p>

<p>The third one is <em>Dynamic Interaction and Uncertainty</em>: Web GUIs are dynamic – pages can change or have pop-ups, and incorrect actions can lead the agent astray. GUI agents need robust error recovery strategies. This has led to new reasoning techniques like reflection and rollback. For example, an agent might try an action, realize it led to an irrelevant page, and then go back and try an alternative – a capability highlighted as essential in recent work [3]. Text agents, while they also handle some dynamic choices (e.g. picking the next link to click), typically operate in a more static information space and have less need for such UI-specific recovery reasoning.</p>

<p>GUI web agents require visual understanding, sequential decision-making, and error-aware reasoning that goes beyond what text-based agents historically needed. These differences have encouraged new research directions to equip GUI agents with better reasoning skills. Before diving into those developments, we next overview the benchmarks and environments that have been created to train and evaluate modern GUI web agents.</p>

<h2 id="core-benchmarks">Core Benchmarks</h2>

<p>In recent years, several benchmarks and agent frameworks have been proposed to evaluate and advance autonomous agents that can interact with websites through a graphical user interface (GUI). These systems combine natural language understanding, web page vision/DOM parsing, and action execution (clicking, typing, etc.) to follow instructions on real or simulated websites. In this section, we review five notable recent projects – VisualWebArena, Mind2Web, Online-Mind2Web, WebVoyager, and RealWebAssist – focusing on their core ideas, evaluation methodologies (tasks, datasets, metrics), key results, and the strengths/limitations of each approach.</p>

<h3 id="visualwebarena">VisualWebArena</h3>

<p>VisualWebArena (VWA) is a benchmark designed to test multimodal web agents on realistic tasks that require visual understanding of web content [4]. Prior web agent benchmarks mostly used text-only information, but VWA includes images and graphical elements as integral parts of the tasks. The goal is to evaluate an agent’s ability to process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. This benchmark builds on the WebArena framework (a realistic web environment) by introducing new tasks where visual content is crucial (e.g. identifying items by images or interpreting graphical page elements).</p>

<p>VWA comprises 910 diverse tasks in three web domains: an online Classifieds site (newly created with real-world data), an e-Shopping site, and a Reddit-style forum. Tasks range from purchasing specific products (which may require recognizing product images) to navigating forums with embedded media. Agents interact with webpages that are self-hosted for consistency. Success is measured by whether the agent achieves the correct end state on the website (functional task completion). Human performance on these tasks is about 88–89% success, indicating the tasks are feasible for people but non-trivial. An example task is: “Find a listing for a red convertible car on the classifieds site and report its price” – requires the agent to visually identify the car’s image/color on the page (beyond just text).</p>

<p>The benchmark supports various agent types. The authors evaluated state-of-the-art large language model (LLM) based agents, including text-only agents and vision-enabled agents (e.g. GPT-4 with vision, open-source vision-language models). Agents observe either a DOM-derived accessibility tree (textual interface) or the rendered webpage image (for vision models), sometimes augmented with image captions (generated by a tool like BLIP-2) for text models. Agents produce actions such as clicking UI elements or entering text. Some experiments also used a Set-of-Mark (SoM) [5] approach – a method to highlight possible interactive elements for the agent to consider – to aid decision making</p>

<p><strong>Key Results</strong>: Text-Only vs Multimodal: A GPT-4 based agent using only text (no vision) achieved under 10% success on VWA tasks (≈7% in one setting). In contrast, GPT-4V (Vision-enabled GPT-4) achieved roughly 15% overall success, more than doubling the success rate by leveraging page images. Even augmenting a text agent with image captions improved success to ~12.8%, but fell short of a true vision model. This underscores that visual context is often critical: purely text-based agents miss cues that are obvious to multimodal agents (e.g. recognizing a product from its photo). The evaluation showed text-only LLM agents often fail when tasks involve identifying visual attributes (colors, shapes, etc.) or non-textual cues. VisualWebArena is the first benchmark to focus on visually-grounded web tasks in a realistic setting. It provides a controlled environment with reproducible websites, enabling rigorous comparison of agents. It highlights the importance of vision in web agents: many real-world web tasks (shopping for an item of a certain style, recognizing icons/buttons, etc.) cannot be solved by text parsing alone.</p>

<h3 id="mind2web">Mind2Web</h3>

<p>Mind2Web is introduced as the first large-scale dataset for developing and evaluating a “generalist” web agent – one that can follow natural language instructions to complete complex tasks on any website [6]. Prior benchmarks often used either simulated web pages or a limited set of websites/tasks, which risked overfitting. Mind2Web instead emphasizes breadth and diversity: it contains over 2,000 tasks collected from 137 real-world websites spanning 31 domains (e-commerce, social media, travel, etc.). Each task is open-ended (expressed in natural language) and comes with a human-crowdsourced action sequence demonstrating how to complete it, providing both a goal specification and a ground-truth solution path.</p>

<h3 id="online-mind2web">Online-Mind2Web</h3>

<p>Online-Mind2Web is a follow-up benchmark that addresses a critical question: Are web agents as competent as some benchmarks suggest, or are we seeing an illusion of progress? [7] The authors of this study argue that previously reported results might be overly optimistic due to evaluation on static or simplified environments. Online-Mind2Web instead evaluates agents in a live web setting, where they must perform tasks on actual, up-to-date websites – thus exposing them to real-world variability and unpredictability. It consists of 300 diverse tasks spanning 136 popular websites, collected to approximate how real users would instruct an agent online. This includes tasks across domains like shopping, travel, social media, etc., similar in spirit to Mind2Web’s diversity but executed on the real web.</p>

<p>The authors developed an LLM-as-a-Judge system called WebJudge to automatically evaluate whether an agent succeeded in a given task, with minimal human labor. WebJudge uses an LLM to analyze the agent’s action history and the resulting web states (with key screenshots) to decide if the goal was achieved. This method achieved about 85% agreement with human evaluators on success judgments, substantially higher consistency than prior automatic metrics, substantially higher consistency than prior automatic metrics.</p>

<h3 id="webvoyager">WebVoyager</h3>

<p>WebVoyager [8] has a set of tasks that are open-ended user instructions that require multi-step interactions. For example, tasks might include “Book a hotel room in New York for next weekend on Booking.com” or “Find and compare the prices of two specific smartphones on an e-commerce site”. The tasks are meant to reflect practical web goals (searching, form-filling, navigating multi-page flows, etc.) and often require combining information gathering with action execution. Because evaluating success on such open tasks can be tricky, WebVoyager’s authors used an automatic evaluation protocol leveraging GPT-4V (similar in spirit to WebJudge). They had GPT-4V examine the agent’s outcome (e.g., final page or sequence of pages visited) and compare it to the intended goal, achieving about 85% agreement with human judgment of success.</p>

<p>Its benchmark and results validated that multimodal agents can substantially outperform text-only agents on real-world websites. Another strength is the introduction of an automatic evaluation metric with GPT-4V to reduce reliance on slow human evaluation</p>

<h3 id="realwebassist">RealWebAssist</h3>

<p>RealWebAssist [9] is a benchmark aimed at long-horizon, sequential web assistance with real users. Unlike one-shot tasks (e.g. “buy X from Amazon”), this benchmark considers scenarios where a user interacts with an AI assistant over an extended session comprising multiple interdependent instructions. The key idea is to simulate a personal web assistant that can handle an evolving to-do list or goal that unfolds through conversation. RealWebAssist addresses several real-world complexities not covered in prior benchmarks. It extends the challenge to interactive, personalized assistance, highlighting the significant gap between current AI capabilities and the full vision of a personal web assistant that can handle ambiguous, evolving user needs over long sessions.</p>

<p>For example, a user might start with “I need a gift for my mother’s birthday”, then later add “She likes gardening” – the agent must adjust its plan. The benchmark includes such evolving goals. The agent might need to learn or keep track of a particular user’s preferences/routines. E.g., “Order my usual Friday takeout” implies knowing what “usual” means for that user. Instructions come from real users (not annotators inventing them), so they reflect personal language quirks and context assumptions.</p>

<p>As with other GUI agents, the assistant must interface with actual web pages (click buttons, fill forms). RealWebAssist emphasizes aligning instructions with the correct GUI actions – e.g., if the user says “open the second result,” the agent must translate that to clicking the second item on the page.</p>

<p>The benchmark consists of sequential instruction episodes collected from real users. For evaluation, the authors consider the following evaluation metrics:</p>
<ul>
  <li>Task success rate: Measures whether the web agent successfully executes all instructions in a given task sequence.</li>
  <li>Average progress: Calculates what percentage of instructions the agent completes correctly before making its first mistake in a task.</li>
  <li>Step success rate: Evaluates the agent’s performance in a controlled setting where it only needs to handle one instruction at a time, with all previous steps assumed to be completed successfully.</li>
</ul>

<h2 id="backbone-vision-language-models-and-ui-grounding">Backbone Vision-Language Models and UI Grounding</h2>

<p>Multimodal LLMs have become the core enablers of GUI agents. Since 2024, a new generation of such models has been adapted to interact with digital interfaces using vision and language. Notably, closed-source giants like OpenAI’s GPT‑4 and Google’s Gemini [17] are pushing capability boundaries. OpenAI’s developed a Computer-Using Agent (CUA) to drive an autonomous GUI agent named <strong>Operator</strong> [18]. This system can visually perceive screens and perform mouse/keyboard actions without specialized APIs. Anthropic’s Claude 3.5 [19] has also gained a “Computer Use” capability, enabling it to “view” a virtual screen and emulate mouse/keyboard interactions. Meanwhile, the community has introduced strong open-source multimodal models to democratize GUI agents. Notably, Alibaba’s Qwen-2.5-VL [20] has delivered impressive visual understanding – e.g. it can handle very large image inputs and precise object localization. Similarly, the ChatGLM family has spawned AutoGLM [21], a specialized multimodal agent model. AutoGLM augments a ChatGLM-based LLM with vision and is trained via reinforcement learning for GUI tasks.</p>

<h3 id="ui-grounding---aligning-perception-and-action">UI Grounding - Aligning Perception and Action</h3>

<p>Achieving fluent GUI automation requires “grounding” the model in the actual UI – i.e. linking the model’s linguistic plans to on-screen elements and executable actions. Recent research has made substantial progress in perception, UI understanding, and action grounding for GUI agents.</p>

<p>Agents must convert a raw GUI (pixels or DOM data) into a form the model can reason about. Two common approaches emerged: (a) leveraging the model’s own multimodal abilities, and (b) inserting an intermediate perception module. The first approach is exemplified by GPT-4V and similar LMMs that directly take screenshots as input. For instance, SeeAct [22] simply feeds the webpage screenshot to GPT-4V and lets it describe and decide actions. The second approach uses computer vision techniques to explicitly identify UI components before involving the LLM. An example is the framework by Song et al. [23], which includes a YOLO-based detector to find UI elements (buttons, text fields, etc.) and read their text via OCR, prior to invoking GPT-4V for high-level planning. Similarly, AppAgent V2 [24] improved over a pure-LLM baseline by integrating OCR and object detection to recognize on-screen widgets, rather than relying on a brittle DOM parser. Microsoft’s UFO agent [25] uses a dual-agent design: one “vision” agent inspects the GUI visually, while another “control” agent queries the app’s UI hierarchy. Agent S [26] likewise feeds both the rendered pixels and the accessibility tree of desktop apps into its LLM module.</p>

<p>After perception, a key challenge is mapping the model’s textual plan (e.g. “Click the OK button, then type the name”) to actual interface actions. SeeAct tested a set-of-candidates prompting strategy where GPT-4V would be asked to mark on the image which element to click. A more robust solution is to instrument the UI with tags or coordinates that the model can reference. Several web agents now render numeric labels next to clickable elements in the screenshot and supply the list of those elements (with their HTML attributes) to the LLM. The LLM’s output can then include the label of the intended element, making execution unambiguous. On mobile UIs, others have used XML UI trees and asked the LLM to match text in the tree; AppAgent V2’s use of detected text tokens is another form of aligning visual info with semantic targets.</p>

<p>Despite this progress, UI grounding remains a core challenge. The consensus in recent literature is that while today’s agents can handle simple cases, complex interfaces still pose problems.</p>

<h2 id="architectures-and-methods-enhancing-reasoning-in-gui-agents">Architectures and Methods: Enhancing Reasoning in GUI Agents</h2>

<p>Developing a general-purpose GUI web agent requires innovation on multiple fronts: how the agent’s policy is represented (LLM-based, reinforcement learned, etc.), how it integrates vision and text, and how it plans and reasons about its next actions. Recent works have explored a spectrum of approaches:</p>

<h3 id="prompt-based-planning-and-chain-of-thought">Prompt-Based Planning and Chain-of-Thought</h3>

<p>Large Language Models, especially since GPT-4 [10] and their open-source equivalents, have become the “brains” of many web agents. Given an instruction and the current page (as text or description of GUI), the LLM can be prompted to output an action (e.g. click(“Add to Cart”)). Early attempts simply used zero-shot or few-shot prompting with GPT-4, but these often lacked deep reasoning for multi-step tasks. To address this, researchers introduced chain-of-thought (CoT) strategies for web agents: the model is encouraged to generate a step-by-step rationale before choosing each action. This was inspired by the ReAct paradigm [11], which interleaves reasoning text (“I need to find a search bar on the page…”) with explicit actions. By reasoning explicitly, the LLM can plan several steps ahead or consider contingencies.</p>

<p>A recent development in 2025 is <strong>WebCoT [3]</strong>, which identifies key reasoning skills for web agents and trains models to use them. WebCoT emphasizes reflection &amp; lookahead, branching, and rollback as three crucial abilities. Instead of relying on an LLM to implicitly learn these, they reconstruct training trajectories with rich rationales that demonstrate, for example, branching (considering multiple possible paths) and rollback (recovering from a bad click) in the chain-of-thought. By fine-tuning an LLM on these augmented trajectories, WebCoT achieved notable gains on WebVoyager, and on tasks like Mind2Web-live and web search QA.</p>

<p>Similarly, other works have looked at incorporating planning abilities directly into the model’s action space. <strong>AgentOccam [2]</strong> by Amazon researchers is one such approach: it introduces special actions like branch and prune which allow the agent to explicitly create and manage a tree of sub-plans. For example, an e-commerce task might require information from two different pages; the agent can branch its plan into two parallel subgoals, gather results, then prune away completed branches. By simplifying the action space (removing distracting or seldom-used actions) and adding these planning operations, AgentOccam achieved a strong performance baseline on the WebArena benchmark.</p>

<p>Another notable result of incorporating better reasoning came from LASER (2024), which tackled WebShop tasks. LASER reframed the web task for the LLM as an explicit state-space exploration problem [12]. Instead of having the LLM treat every step as unrelated to the next, LASER defines distinct states (e.g. “search results page” or “product page”) and allowed actions from each state, enabling the model to backtrack and explore alternatives. This approach acknowledged that an LLM agent may make a mistake, but it can recover by returning to a previous state and trying a different path (much like a human clicking the back button and trying the next link). The impact of this reasoning-aware formulation was striking on e-commerce tasks – LASER nearly closed the gap with human performance on WebShop [13].</p>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<p>While prompting and supervised fine-tuning have seen success, another thread of research focuses on learning from experience to improve GUI agents. The idea is that by actually trial-and-error interacting with the environment, an agent can learn a policy that is more robust than one purely imitated from limited data. However, applying RL to large language model-based agents is non-trivial (due to huge action spaces, long horizons, and the expense of running many trials). Recent work has made progress on this front:</p>

<p><strong>WebAgent-R1 (2025)</strong> [14] is an end-to-end RL framework that achieved state-of-the-art performance on WebArena by carefully adapting RL algorithms to LLM agents. The authors designed a reward function that encourages the agent to complete the task at hand while also exploring the web. They also introduced a method to handle the sparse and delayed reward signal present in web navigation tasks. One innovation in WebAgent-R1 is a dynamic context compression mechanism: as an episode proceeds through dozens of pages, the text observation can become thousands of tokens long (exceeding model limits). WebAgent-R1 dynamically truncates or compresses less relevant context from earlier steps to keep the input size manageable. Another contribution is extending GRPO [15] to multi-turn settings (called M-GRPO) and using asynchronous parallel browsing to collect experiences faster.</p>

<p>Another angle of learning from experience is self-improvement via trajectory relabeling or self-play. Researchers have explored letting an LLM agent run on many tasks, then analyzing its failures and successes to generate additional training data [16]. The agent would attempt tasks, then (using the LLM’s own evaluation or a heuristic) identify where it went wrong, and synthesize corrected trajectories or critiques. These synthetic trajectories, added to a finetuning dataset, boosted the task completion rate by ~31% over the base model on WebArena. Such self-refinement approaches are appealing because they mitigate the need for large human-labeled datasets – the agent essentially learns from its mistakes. However, they also require careful validation to ensure the agent doesn’t reinforce bad behaviors.</p>

<h2 id="replication-study-evaluating-qwen-25-vl-as-a-gui-agent">Replication Study: Evaluating Qwen-2.5-VL as a GUI Agent</h2>

<h2 id="conclusion">Conclusion</h2>

<h2 id="references">References</h2>

<p>[1] Nakano, Reiichiro, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).</p>

<p>[2] Yang, Ke, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. “Agentoccam: A simple yet strong baseline for llm-based web agents.” arXiv preprint arXiv:2410.13825 (2024).</p>

<p>[3] Hu, Minda, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, and Irwin King. “WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback.” arXiv preprint arXiv:2505.20013 (2025).</p>

<p>[4] Koh, Jing Yu, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. “Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.” arXiv preprint arXiv:2401.13649 (2024).</p>

<p>[5] Yang, Jianwei, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. “Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.” arXiv preprint arXiv:2310.11441 (2023).</p>

<p>[6] Deng, Xiang, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. “Mind2web: Towards a generalist agent for the web.” Advances in Neural Information Processing Systems 36 (2023): 28091-28114.</p>

<p>[7] Xue, Tianci, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, and Yu Su. “An illusion of progress? assessing the current state of web agents.” arXiv preprint arXiv:2504.01382 (2025).</p>

<p>[8] He, Hongliang, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. “WebVoyager: Building an end-to-end web agent with large multimodal models.” arXiv preprint arXiv:2401.13919 (2024).</p>

<p>[9] Ye, Suyu, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, and Tianmin Shu. “Realwebassist: A benchmark for long-horizon web assistance with real-world users.” arXiv preprint arXiv:2504.10445 (2025).</p>

<p>[10] Achiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida et al. “Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).</p>

<p>[11] Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. “React: Synergizing reasoning and acting in language models.” In International Conference on Learning Representations (ICLR). 2023.</p>

<p>[12] Ma, Kaixin, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. “Laser: Llm agent with state-space exploration for web navigation.” arXiv preprint arXiv:2309.08172 (2023).</p>

<p>[13] Yao, Shunyu, Howard Chen, John Yang, and Karthik Narasimhan. “Webshop: Towards scalable real-world web interaction with grounded language agents.” Advances in Neural Information Processing Systems 35 (2022): 20744-20757.</p>

<p>[14] Wei, Zhepei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu et al. “WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning.” arXiv preprint arXiv:2505.16421 (2025).</p>

<p>[15] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300.</p>

<p>[16] Patel, Ajay, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. “Large language models can self-improve at web agent tasks.” arXiv preprint arXiv:2405.20309 (2024).</p>

<p>[17] Team, Gemini, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk et al. “Gemini: a family of highly capable multimodal models.” arXiv preprint arXiv:2312.11805 (2023).</p>

<p>[18] OpenAI. 2024. Introducing Operator. https://openai.com/index/introducing-operator/</p>

<p>[19] Anthropic. 2024. Anthropic’s new Claude 3.5 Sonnet model is now available. https://www.anthropic.com/news/claude-3-5-sonnet</p>

<p>[20] Bai, Shuai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang et al. “Qwen2. 5-vl technical report.” arXiv preprint arXiv:2502.13923 (2025).</p>

<p>[21] Liu, Xiao, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao et al. “Autoglm: Autonomous foundation agents for guis.” arXiv preprint arXiv:2411.00820 (2024).</p>

<p>[22] Zheng, Boyuan, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. “Gpt-4v (ision) is a generalist web agent, if grounded.” arXiv preprint arXiv:2401.01614 (2024).</p>

<p>[23] Yunpeng Song, Yiheng Bian, Yongtao Tang, and Zhongmin Cai. 2023. Navigating Interfaces with AI for Enhanced User Interaction. ArXiv:2312.11190</p>

<p>[24] Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. 2024b. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824.</p>

<p>[25] Zhang, Chaoyun, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma et al. “Ufo: A ui-focused agent for windows os interaction.” arXiv preprint arXiv:2402.07939 (2024).</p>

<p>[26] Agashe, Saaket, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. “Agent s: An open agentic framework that uses computers like a human.” arXiv preprint arXiv:2410.08164 (2024).</p>]]></content><author><name>Genglin Liu (Student 27), James Shiffer (Student 25)</name></author><summary type="html"><![CDATA[Web agents are a new class of agents that can interact with the web. They are able to navigate the web, search for information, and perform tasks. They are a type of multi-modal agent that can use text, images, and other modalities to interact with the web. Since 2024, we have seen a surge in the development of web agents, with many new agents being developed and released. In this blog post, we survey the recent developments in the field of web and particularly GUI agents, and provide a comprehensive overview of the state of the art. We review core benchmarks - WebArena, VisualWebArena, Mind2Web, and AssistantBench — that have enabled systematic measurement of these capabilities. We discuss the backbone vision-language models that power these agents, as well as the recent advancement in reasoning.]]></summary></entry><entry><title type="html">Peek-A-Boo, Occlusion-Aware Visual Perception through Active Exploration</title><link href="http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo.html" rel="alternate" type="text/html" title="Peek-A-Boo, Occlusion-Aware Visual Perception through Active Exploration" /><published>2024-12-13T00:00:00-08:00</published><updated>2024-12-13T00:00:00-08:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo</id><content type="html" xml:base="http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo.html"><![CDATA[<blockquote>
  <p>In this study, we present a framework for enabling
robots to locate and focus on objects that are partially or fully
occluded within their environment. We split up any robotic tasks
into two steps: Localization, where the robot searches for objects
of interest, and Task Completion, where the robot completes the
task after finding the object. We propose Peekaboo, a solution
to the Localization stage to find partially or even fully occluded
objects. We train a reinforcement learning algorithm to teach
the robot to actively reposition its camera to optimize visibility
of occluded objects. The key features include engineering a
reward function that incentivizes effective object localization and
setting up a comprehensive training environment. We develop
a simulation environment with randomness to learn to localize
from numerous initial viewpoints. Our approach also includes
the implementation of a vision encoder for processing visual
input, which allows the robot to interpret and respond to
objects and occlusions. We design metrics to quantify the model’s
performance, demonstrating its capability to handle occlusions
without any human intervention at all. The results of this work
showcase the potential for robotic systems to actively improve
their perception in cluttered or obstructed environments.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#prior-works" id="markdown-toc-prior-works">Prior Works</a></li>
  <li><a href="#problem-formulation" id="markdown-toc-problem-formulation">Problem Formulation</a></li>
  <li><a href="#proposed-methods-and-evaluation" id="markdown-toc-proposed-methods-and-evaluation">Proposed Methods and Evaluation</a></li>
  <li><a href="#experiments" id="markdown-toc-experiments">Experiments</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ul>
      <li><a href="#future-work" id="markdown-toc-future-work">Future Work</a></li>
    </ul>
  </li>
  <li><a href="#links" id="markdown-toc-links">Links</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In dynamic and cluttered environments, robotic systems
with visual perception capabilities must be able to adapt their
viewpoints to maintain visibility of target objects, even when
occlusions or obstacles obstruct their line of sight. Traditional
robotic vision systems often address this requirement by either
using a large array of static sensors, usually cameras, pointing in different orientations, or by moving a single
sensor in a predetermined path. Both approaches have their
downsides. Arrays of static sensors are far more expensive than
using a single sensor, and rely on the motion of their agent
to acquire new viewpoints. Sensors that follow predetermined
paths lack the flexibility to capture environment-dependent
information about complex scenes common in real-world
settings. These limitations serve as critical bottlenecks slowing
the advancement of vision-based robotics. Active Vision is a
promising field that aims to address these challenges. The goal
of Active Vision is to mimic the way that humans perceive
their environment: by learning to move and orient a single
sensor in ways that capture information important for completing some task.</p>

<p>Active Vision avoids the shortcomings of
the other two approaches; the agent uses a single controllable
sensor, as opposed to an array of fixed sensors to perceive
its environment, and the agent can learn to manipulate its
sensor in response to its environment in nuanced ways that
a predetermined approach could not.</p>

<p>Recent advances in reinforcement learning (RL) have looked
into enabling robots to learn behaviors directly from their
interactions with the environment, making it possible to train
autonomous systems to explore the environment, allowing
for active perception. In robotic vision tasks, RL algorithms
can enable a camera mounted on a robotic arm to not only
locate objects but also to continuously adjust its position
to avoid occlusions and improve object visibility. However,
developing such an RL framework requires overcoming several
challenges, including creating a robust training environment
that mimics the need to shift camera perspectives, and engineering reward functions that incentivize behavior promoting
consistent visibility.</p>

<p>We propose Peekaboo, an approach that addresses many
of the shortcomings of previous methods. Peekaboo’s key
insight is that the Localization and Task Completion steps in
manipulation tasks can be decoupled with minimal loss of
generality. In practice, when humans perform manipulation
tasks, we search our environment for an object before extending a hand in its direction to grasp it. The same logic applies here. Any manipulation task first requires the agent
to localize the object, along with anything else critical for
completing the task. Peekaboo focuses on this Localization
step, but unlike previous works is designed to be robust to
very heavy occlusions, and allows the camera to be controlled
with many degrees of freedom.</p>

<h2 id="prior-works">Prior Works</h2>

<p>Prior works in Active Vision address similar problems to
ours, but all differ in some key areas. While there are many
works in Active Vision, we mention those most relevant to
our method. A recent approach by researchers at Google
Deepmind proposes using navigation data to enhance the
performance of cross-embodiment manipulation tasks [8]. The
researchers demonstrate that learning a general approach to
navigation from sources using different embodiments is key
to performing manipulation tasks. Their findings reinforce the
notion that navigation is a robotics control primitive that can
aid any task, even those that do not explicitly perform navigation.</p>

<p>Other works like those by researchers from the University of Texas at Austin have proposed performing complex
tasks using an “information-seeking” and an “information-receiving” policy to guide the search task and manipulation task separately [3]. While impressive, their approach Learning
to Look is designed to perform complex manipulation tasks in
simple unobstructed environments with limited camera motion.
In contrast, our goal is to operate in environments designed to
hinder search tasks and force the agent to substantially move
its camera to succeed. There has also been plenty of work
in using Active Vision as a tool to improve performance in
well-studied tasks like grasping [6] and object classification
[7]. More recently, there has been an increased focus on
learning Active Vision policies from human demonstrations
using Imitation Learning. Ian Chuang et al. propose a method
that allows a human teleoperator to control both a camera and
a robotic arm in a VR environment in order to collect near-optimal human demonstration data of Active Vision tasks like inserting a key into a lock.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/pworksfig1.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Using learned Active Vision policies as a method to acquire novel views of objects to aid in object classification.</em></p>
</div>

<p>Two prior works stand out as particularly similar to our task
and deserve additional attention. The first approach by Stanford researchers, DREAM, proposes decoupling their agent’s
task into an explorative navigation step and exploitative task
completion stage, each of which is learned separately [5]. This
approach relies on an intrinsic task ID, and leverages memory
from previous explorative iterations through a memorization
process to aid it in the current iteration. While the researchers
demonstrate impressive results, we find that their environment
is somewhat limited. Since the researchers are performing
navigation, their agent is limited to moving in two degrees
of freedom, and rotating in a single degree of freedom. In
contrast, our approach freely manipulates the agent in six
degrees of freedom. In addition, their task is designed such
that the agent can physically access the entire environment:
an assumption that is often not true in physical systems in the
real world.</p>

<p>Another highly relevant paper from researchers
at Carnegie Mellon uses Active Vision to aid a manipulation
task in the presence of visual occlusions [1]. The authors
propose a task of pushing a target cube onto a target coordinate
using a robotic arm, where the agent learns to independently
manipulate its robotic arm and the camera from which it sees
to avoid physical occlusions preventing task completion. Like
DREAM, the proposed method limits the agent to few degrees
of freedom in its motion. In addition, the agent operates on
fairly “weak” occlusions that rarely impede task completion.
Lastly, the authors learn a single policy that jointly manipulates
the robotic arm and controls the camera – a process we
believe can be decoupled.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/pworks2.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Learning to perform manipulation in the presence of visual occlusions by learning to control the camera. The agent jointly learns to control the robotic arm to move the cube onto the target, and control the camera to avoid visual obstructions.</em></p>
</div>

<h2 id="problem-formulation">Problem Formulation</h2>

<p>We contribute a novel task formulation unique to Peekaboo,
and central to performing occlusion-aware search. We train
Peekaboo using a Reinforcement Learning framework, which
requires us to define three components: the environment, the
agent, and the reward function.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/mdp.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Markov Decision Process (MDP) Framework</em></p>
</div>

<p>The environment is a simple indoor room, containing a
central table. On the table lies a randomly positioned target
cube and a large randomly positioned wall meant to block the
view of the cube.</p>

<p>The agent is a 6-DoF Panda robotic arm, initialized to look
in a random direction. It can move its end effector in any
direction, and rotate its end effector to any orientation. The
agent observes its environment through a sensor in its hand
that captures images from the hand’s point of view.
The reward function is a binary reward function. The value
of the reward is one if the target cube is within the frame of
the robotic arm’s camera observation. Otherwise, if the cube
is off-frame, or occluded by the wall, the value of the reward
is zero. This reward function encourages the agent to search
its environment until it can see the target cube.
Using this particular task, we aim to train Peekaboo such
that it can learn to search around occlusions to locate the target
cube, regardless of variations in the scene.
In summary, we focus on building a flexible, RL-based
framework that enables a robotic arm-mounted camera to
autonomously determine the optimal perspectives for tracking
objects, particularly in environments where occlusions are
frequent. To achieve this, we will:</p>

<ol>
  <li><strong>Introduce randomness</strong>
    <ul>
      <li>Randomize both the environment and camera settings during the training phase.</li>
      <li>Ensure the model is robust to variable conditions.</li>
    </ul>
  </li>
  <li><strong>Include a vision encoder</strong>
    <ul>
      <li>Process incoming visual data effectively.</li>
    </ul>
  </li>
  <li><strong>Incorporate a reinforcement learning (RL) algorithm</strong>
    <ul>
      <li>Train the camera’s decision-making process.</li>
    </ul>
  </li>
  <li><strong>Engineer a reward function</strong>
    <ul>
      <li>Emphasize maintaining clear sightlines.</li>
      <li>Penalize occlusions to improve performance.</li>
    </ul>
  </li>
</ol>

<p>This work contributes to the growing field of adaptive
robotic vision, offering a method for giving robots autonomous, context-aware vision capabilities that can support
applications requiring real-time adaptability in unpredictable
environments.</p>

<h2 id="proposed-methods-and-evaluation">Proposed Methods and Evaluation</h2>
<p>We propose a two stage RL training framework that allows
our robot to first search for and focus on the object of interest,
and then complete the desired task by interacting with the
object. The focus of our methods will be on training and
testing the first stage of this framework, as there are many
prior works that focus on task completion. We will refer to
this first stage as Localization and the second stage as Task
Completion. The Localization phase will consist of training
an RL agent that learns to move an egocentric, or first person
view, camera to search around occlusions by exploring the
environment, with the end goal of localizing the object of
interest in its frame of view.</p>

<p>We will approach this problem using a custom environment
in Robosuite. Our environment will be built off of the base
Lift task environment, with modifications made to create
occlusions. Specifically, we will put a wall in between the
robotic arm and the cube that the arm is trying to lift. This
will be the main occlusion that is dealt with in our Wall task.
In its initial frame observation of the environment, the robot
will not be able to see the cube it is trying to lift, as it will be
covered by the wall. We also introduced cube randomization,
camera randomization, and arm randomization. The cube and
arm randomization was done through the Robosuite framework. The camera and wall randomization relied on custom
quaternions that changed the orientation of our objects. The
goal of the overall task is for the robot to search for the cube,
find it behind the wall, and then lift it.</p>

<p>Through our two stage framework, we propose a MDP
formulation for the Localization stage. The state space
consists of the image observations from our robot. We will
have a camera attached the end effector of the robotic arm
to observe these images. The robot will not have access to
ground truth proprioceptive data. The action space consists
of modifying the perspective of the camera, which is done
through the Cartesian coordinates of moving the robotic arm.
The reward function will be as follows:</p>

\[\text{reward} = \begin{cases} 
      0 &amp; \text{if not in view} \\
      1 &amp; \text{if in view} 
   \end{cases}\]

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig1.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 1. Visualization of our Reward Function</em>.</p>
</div>

<p>If the cube is in view, we get a reward of 1, and if it is not, the reward is 0. We will implement this in the environment using the ground truth proprioceptive data of the robotic arm, the wall, and the cube. Using this position data, we can calculate the angle in between the robotic arm and wall, and the robotic arm and the cube. These angles can then indicate if the wall is blocking the robotic arm’s frame of view from the cube or not, which allows us to return a reward of 0 or 1. Given that the robot does not have access to this ground truth data, we are essentially teaching the robot to recognize and search around occlusions when they show up prominently in its camera frame.</p>

<p>Here is a snippet from our code:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def reward(obs):
    target_vertices, wall_vertices, camera_position, camera_vec, camera_bloom = preprocess(obs)

    # if any corner of the target cube is outside of frame, return 0
    for target_vertex in target_vertices:
        if not target_visible_in_conical_bloom(target_vertex, camera_position, camera_vec, camera_bloom):
            return 0

    # if any corner of the target cube is blocked by the wall, return 0
    for target_vertex in target_vertices:
        for wall_plane in wall_vertices:
            if line_segment_intersects_truncated_plane(target_vertex, camera_position, wall_plane):
                return 0
        
    # cube is entirely within bloom and is not obstructed
    return 1
</code></pre></div></div>

<p>Once we have set up the Robosuite environment with the camera and reward function, we will train a PPO agent on the MDP formulation of the \(\textit{Localization}\) stage. The image observations will be processed by a pretrained Vision Encoder, which will be frozen during training. The output features of the Vision Encoder will be the inputs to our RL neural networks. Our evaluation will consist of visualizing rollouts and establishing a success rate for the \(\textit{Localization}\) phase. A rollout will be considered a success if the object of interest is seen completely unobstructed in the scene from the perspective of the robot. In order to test for generalization, we will randomize initial states of the robotic arm, the wall, and the cube. Sometimes, the cube will be completely in view, and sometimes it will be completely occluded. It will be up to the robot to learn and decide when it needs to search around the wall and when it has the object of interest in sight.</p>

<p>Our main contributions are the training and evaluation of this first stage. Now that we have run randomized tests, we have set up a foundation to implement the second stage of our proposed method. We can simply use the final, localized state of our first stage as the initial state for our \(\textit{Task Completion}\) stage, which will also be trained using the PPO algorithm. The goal then would be to first use the \(\textit{Localization}\) stage to find the desired object behind any occlusions, and then use the \(\textit{Task Completion}\) stage to execute the task. We can then visualize rollouts of this end to end pipeline and determine the success rate.</p>

<p>This two stage training can then be compared against a baseline of training one PPO agent for the entire task from scratch. In effect we want to show that while one agent is not able to both find the object and finish the task in one go, our two step approach is able to successfully break this down into multiple steps.</p>

<h2 id="experiments">Experiments</h2>
<p>Our experimental environment is in Robosuite. We use a custom Robosuite environment, built off their Lift Environment implementation. This environment provides a target cube object, a robotic arm, a mounted camera on the robotic arm, and a table. We customize this class by adding randomness to the initial positions and orientations of all of the above objects except the table. We also add our own wall, randomized in size, positioning, and orientation. Fig. 2 shows the top down view of our experiment setup. This figure is for visual understanding purposes only and the images from this camera are not used in our training. Fig. 3 shows the first person images from our mounted camera, randomized at each initialization. We use images from this camera to train the arm to move the camera to place the target object in frame. For our training runs, we can train models with varying degrees of randomization. Specifically, in this project, we experimented with turning camera randomization on and off. For all of our training runs, we kept wall and cube randomization always on.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig2.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 2. Examples of Top down view of randomized initialization</em>.</p>
</div>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig3.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 3. Examples of First person view of randomized initialization</em>.</p>
</div>

<p>For our training process, we interpret the camera image using DINOv2, a vision transformer based vision encoder, and we feed the results into our reward function. This vision encoder takes in images of size 224 by 224 from the environment and outputs 384 features to our RL policy.</p>

<p>We use a proximal policy optimization (PPO) reinforcement learning algorithm that trains our model to look at the target object. We initialize our RL policy to be a 2 layer MLP with 256 activations per layer, taking the feature inputs from our vision encoder and outputing the action for our robotic arm. We configure this model through the Stable-Baselines3 library. Ideally, we would like to train our policy for 1 million timesteps, which amounts to 2000 episodes, but due to training time constraints, we begin with 200k timesteps, which amounts to 400 episodes. Further discussion of training is presented in our results.</p>

<p>Our metrics for the model that we trained are the rewards that we defined earlier. We set the episode horizon to 500, meaning that there is a total possible reward per episode of 500. A lower reward indicates that the policy struggled to find the cube. The closer the reward is to 500 the better, indicating that our policy was able to localize the cube early in the episode, and learned to keep it within view the entire time.</p>

<h2 id="results">Results</h2>

<p>For our results, we evaluate two main training runs to demonstrate the performance of our method. As we are working with a completely custom environment and task, we are evaluating the performance of an approach to a new problem formulation.</p>

<p>Initially, we trained a model on our fully randomized environment, including randomization of the cube, wall, and camera angle. The reward graph for the training of this model is presented in Figure 4. The model is trained for 200k timesteps, which is about 400 episodes, and took approximately 10 hours to train. We can see that the general trend of the rewards is upwards, with an increase in rewards at around the 130k timesteps mark. However, the rewards are also oscillating after that point, showing variability and lack of convergence to our results.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig4.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 4. Rewards Graph for training with randomization of wall, cube and
camera</em>.</p>
</div>

<p>We wanted to demonstrate more stability within our results, so the next model we trained was without camera randomization. The reward graph for this training run is presented in Figure 5. We see an increase in rewards earlier in this graph at 90k timesteps, but then it drops again before picking up towards the end of training. Again, we see that our rewards do not converge.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig5.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 5. Rewards Graph for training with randomization of only wall and cube</em>.</p>
</div>

<p>In order to test if the model converges, we decided to continue training the model with full environment randomization (camera, wall, and cube) for 1 million time steps, which took 40+ hours. Unfortunately, this training run did not converge either, and showed the same up and down performance that our shorter training runs displayed. Therefore, we see that naively training the model for longer does not improve performance.</p>

<h2 id="discussion">Discussion</h2>
<p>When we tested our model after training, we saw that our policy’s actions greatly depends on the initial conditions of the task. Since we are randomizing the environment before every episode, the initial conditions could vary from episode to episode. In the case that the wall or cube are in view in the beginning, the model performs reasonably by searching around them. However, we also see many cases when the robotic arm is pointed in an arbitrary direction away from the wall and cube. In this case, it sees just a blank white image of the table. This initial observation gives the robot no information with which to find the cube or even start searching, since that image is the only observation (we do not give the robot proprioceptive data). There is no indication within the image which way it should even search, as that would change from episode to episode due to environment randomization. We believe that this variance in the task causes instability in performance, which aligns with the results we are seeing. When there is a favorable initial condition, the model learns well, but when there is no useful information in the initial condition, performance declines and causes instability in  rewards. This performance would explain why our training graphs look the way they do, and why training does not converge simply with training for longer.</p>

<p>It is also important to note that even though we trained for 10+ hours, that only amounted to 400 episodes, which is not very much compared to other environments that train RL models with training episodes in the thousands. This increase in compute goes back to our problem setup, where we settled on the computationally expensive DinoV2 vision encoder with high resolution images, and set the complexity of our task through drastic randomization of our task environment. In future works, especially with a visually simple task environment (the only objects are a wall and cube, with no visual intricacies) it may have been better to use a smaller Vision Encoder like a ResNet or train our own CNNPolicy with lower resolution images. Additionally, we had hoped that the exploration of RL would allow it to learn to overcome drastic randomization, but through our results it seems that we were potentially wrong. It may have been better to start with no randomization and just a wall occluding a cube, and once that was working, we could incrementally add randomization to increase complexity. Once these changes are implemented, we believe that our solution could be a viable solution to the active exploration problem that we are working on.</p>

<h2 id="conclusion">Conclusion</h2>
<p>We present Peek-A-Boo, a two stage Reinforcement Learning framework that aims to break down any task into two stages: Localization/Search and Task Completion. We create a custom task environment with occlusions and randomization, an engineered reward function, and train an RL model with a Visual Encoder to complete the Localization stage. Through our model training, we are able to demonstrate promise in our approach through generally increasing rewards despite variability in training performance.</p>

<h3 id="future-work">Future Work</h3>
<p>Future work would focus on simplifying the task with less drastic randomization to show stable performance. Then, we can scale up to more complicated tasks. Future work could also look into other ways to stabilize the performance of the model. For instance, the Localization stage of our framework could be approached using IBRL (Imitation Bootstrapped Reinforcement Learning) [4] in order to improve stability through Imitation Learning while allowing the model to also explore the environment through RL. Additionally, another avenue of future work could be automating the reward function using object detection or semantic segmentation on the observation image. We have engineering a reward function using ground truth data, but that may not always be available when applying this framework.</p>

<p>Overall, we see this as a promising first step for active perception and occlusion-aware robotics. We are excited to see the advancements in this field to hopefully one day see robot policies that are able to explore and generalize to any extenuating circumstance that they face in their environment.</p>

<h2 id="links">Links</h2>
<p><a href="https://github.com/ophirsim/Peekaboo">Link to our codebase</a></p>

<p>Custom feature extractor using the pretrained DINOv2 base model from timm: <a href="https://github.com/ophirsim/Peekaboo/blob/main/vision_encoder.py">Link</a></p>

<p>Stable Baselines Documentation: <a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">Link</a></p>

<h2 id="references">References</h2>
<p>[1] Cheng, R., Agarwal, A., &amp; Fragkiadaki, K.
(2018). Reinforcement Learning of Active Vision
for Manipulating Objects under Occlusions.
Conference on Robot Learning, 422-431.
http://proceedings.mlr.press/v87/cheng18a/cheng18a.pdf</p>

<p>[2] Chuang, I., Lee, A., Gao, D., &amp; Soltani, I. (2024). Active
Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation. arXiv preprint
arXiv:2409.17435.</p>

<p>[3] Dass, S., Hu, J., Abbatematteo, B., Stone, P., &amp; Martin, R. (2024). Learning to Look: Seeking Information for Decision Making via Policy Factorization. arXiv
preprint arXiv:2410.18964.</p>

<p>[4] Hu, H., Mirchandani, S., &amp; Sadigh, D. (2024). Imitation
Bootstrapped Reinforcement Learning. arXiv [Cs.LG].
Retrieved from http://arxiv.org/abs/2311.02198</p>

<p>[5] Liu, E. Z., Finn, C., Liang, P., &amp; Raghunathan, A.
(2021, November 12). Decoupling exploration and exploitation in meta-reinforcement learning without sacrifices. Exploration in Meta-Reinforcement Learning.
https://ezliu.github.io/dream/</p>

<p>[6] Natarajan, S., Brown, G., &amp; Calli, B. (2021).
Aiding Grasp Synthesis for Novel Objects Using Heuristic-Based and Data-Driven Active Vision Methods. Frontiers in Robotics and AI, 8.
https://doi.org/10.3389/frobt.2021.696587</p>

<p>[7] Safronov, E., Piga, N., Colledanchise, M., &amp;
Natale, L. (2021, August 2). Active perception
for ambiguous objects classification. arXiv.
https://arxiv.org/pdf/2108.00737.pdf</p>

<p>[8] Yang, J., Glossop, C., Bhorkar, A., Shah, D., Vuong,
Q., Finn, C., … &amp; Levine, S. (2024). Pushing the
limits of cross-embodiment learning for manipulation
and navigation. arXiv preprint arXiv:2402.19432.</p>

<hr />]]></content><author><name>Medha Kini, Ophir Siman-Tov</name></author><summary type="html"><![CDATA[In this study, we present a framework for enabling robots to locate and focus on objects that are partially or fully occluded within their environment. We split up any robotic tasks into two steps: Localization, where the robot searches for objects of interest, and Task Completion, where the robot completes the task after finding the object. We propose Peekaboo, a solution to the Localization stage to find partially or even fully occluded objects. We train a reinforcement learning algorithm to teach the robot to actively reposition its camera to optimize visibility of occluded objects. The key features include engineering a reward function that incentivizes effective object localization and setting up a comprehensive training environment. We develop a simulation environment with randomness to learn to localize from numerous initial viewpoints. Our approach also includes the implementation of a vision encoder for processing visual input, which allows the robot to interpret and respond to objects and occlusions. We design metrics to quantify the model’s performance, demonstrating its capability to handle occlusions without any human intervention at all. The results of this work showcase the potential for robotic systems to actively improve their perception in cluttered or obstructed environments.]]></summary></entry></feed>