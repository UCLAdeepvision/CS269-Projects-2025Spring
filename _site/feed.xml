<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/CS269-Projects-2025Spring/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/CS269-Projects-2025Spring/" rel="alternate" type="text/html" /><updated>2025-06-10T17:13:25-07:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/feed.xml</id><title type="html">2025S, UCLA CS269 Course Projects</title><subtitle>Course projects for UCLA CS269, Seminar on AI Agents and Foundation Models</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Neural Game Engine</title><link href="http://localhost:4000/CS269-Projects-2025Spring/2025/06/10/student-19-NeuralGameEngine.html" rel="alternate" type="text/html" title="Neural Game Engine" /><published>2025-06-10T00:00:00-07:00</published><updated>2025-06-10T00:00:00-07:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/2025/06/10/student-19-NeuralGameEngine</id><content type="html" xml:base="http://localhost:4000/CS269-Projects-2025Spring/2025/06/10/student-19-NeuralGameEngine.html"><![CDATA[<blockquote>
  <p>This report explores recent advances in neural game engines, with a focus on generative models that simulate interactive game environments. Based on the prior presentation of GameNGen, a diffusion-based video model trained on DOOM, I review and compare several state-of-the-art approaches including DIAMOND, MineWorld, IRIS, GameGAN, and the original World Models. I also analyze their differences in architecture, visual fidelity, speed, and controllability, highlighting the trade-offs each design makes. Finally I conclude with a discussion on future directions for building responsive, efficient, and generalizable neural simulators for reinforcement learning and interactive media.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#prior-works" id="markdown-toc-prior-works">Prior Works</a></li>
  <li><a href="#comparison" id="markdown-toc-comparison">Comparison</a>    <ul>
      <li><a href="#methodology" id="markdown-toc-methodology">Methodology</a></li>
      <li><a href="#comparison-table" id="markdown-toc-comparison-table">Comparison Table</a></li>
      <li><a href="#advantages-and-disadvantages" id="markdown-toc-advantages-and-disadvantages">Advantages and Disadvantages</a></li>
    </ul>
  </li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a>    <ul>
      <li><a href="#conclusion-and-future-work" id="markdown-toc-conclusion-and-future-work">Conclusion and Future Work</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In the earlier presentation, we have explored the GameNGen together, a neural game engine built using a diffusion-based video model. The goal of GameNGen is to generate realistic gameplay sequences from pixel inputs, simulating game environments without relying on traditional physics or rule-based engines. One of the key ideas was to use a powerful video diffusion model that could produce rich and coherent game visuals, allowing agents to interact with imagined worlds in a way that looks visually convincing.</p>

<p>Before these recent advances, earlier models like World Models, GameGAN, and IRIS laid the foundation by either simulating in compact latent spaces or using GANs and transformers for visual prediction. However, they struggled with long-term consistency, visual detail, or speed—motivating the newer generation of neural game engines I focus on in this report.</p>

<p>While GameNGen showed promising results, especially in terms of visual quality, it also brought up some important limitations. For example, it struggled with speed due to the heavy diffusion process (achieved approximately 20 FPS), and had trouble maintaining long-term consistency in gameplay. These issues sparked a lot of interest in the community, and several newer models have tried to tackle these challenges from different angles.</p>

<p>One of these new attempts is DIAMOND, which also uses diffusion for world modeling. But this model focuses more on action-conditioning and control. It shows that adding extra loss terms and guiding the model through better conditioning can help it generate more meaningful sequences, not just pretty. DIAMOND also runs experiments in classic environments like Atari games and demonstrates measurable improvements in decision-making performance, thanks to the improved visual details.</p>

<p>Besides, MineWorld goes in a different direction. Instead of diffusion, it uses an autoregressive transformer model to simulate future frames. In the Q&amp;A part of my presentation, we discussed the diffculties for extending GameNGen to AAA games like Minecarft or Apex. Suprisingly, MineWorld has achieved this goal. Compared with DOOM for GameNGen, Mineworld simulates in Minecraft, which is much more complex. It takes both visual frames and player actions, turns them into discrete tokens, and feeds them into a transformer that predicts what happens next. Based on a parallel decoding strategy, MineWorld can generate multiple frames per second, which makes it much more usable for real-time interactions.</p>

<p>All these models — GameNGen, DIAMOND, MineWorld — are part of a growing trend where generative models are being used not just to produce content, but to simulate interactive environments. This shift has major implications for reinforcement learning, robotics, and even game development. In this report, I will explore how these models differ in terms of architecture, speed, visual quality, and controllability.</p>

<h2 id="prior-works">Prior Works</h2>
<p>The idea of learning a model to simulate environments, also known as world modeling, has been explored for years in reinforcement learning and generative modeling. Early work like Ha and Schmidhuber’s World Models (2018) introduced a framework that combined a variational autoencoder (VAE), a recurrent world model, and a controller, allowing agents to learn entirely within their own imagination. While this approach was conceptually powerful, it relied heavily on low-dimensional latent states, which means that much of the visual richness and contextual information was lost during compression. Later efforts such as IRIS (2022) built on this by introducing discrete latents via vector quantization and modeling sequences with Transformers, achieving strong results on Atari with limited data. However, these models still suffered from the trade-off between visual fidelity and computational efficiency, often missing important details like small sprites or dynamic elements critical to gameplay decisions.</p>

<p>Personally I would say that the introduction of GameGAN (2020) could be a milestone, which used a GAN-based approach to learn how to simulate entire environments directly from visual observations and player actions. While impressive for its time, GameGAN still lacked generalization capabilities and tended to fail on relatively long horizon rollouts or 3D environments. That’s where GameNGen entered the scene. Instead of compressing the environment into a minimal latent space or relying on GANs, GameNGen employed diffusion models, which was originally designed for high-quality image synthesis, to generate video game frames in an autoregressive fashion. By training on gameplay data from DOOM and leveraging a modified Stable Diffusion model, GameNGen could generate realistic, temporally coherent frames based on previous context and action inputs. It introduced several techniques to stabilize the generation process, including noise augmentation during training and decoder fine-tuning for UI elements. The result was a neural “game engine” capable of producing video sequences at a surprisingly high quality, even reaching up to 20 FPS on a TPU. However, its main limitations were relatively high computational costs and limited memory, which affected both real-time interaction and long-horizon consistency.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/student-19/figure1.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Figure 1. GameGAN: If you look at the person on the top-left picture, you might think she is playing Pacman of Toru Iwatani, but she is not! She is actually playing with a GAN generated version of Pacman. In this paper, GameGAN was introduced that learns to reproduce games by just observing lots of playing rounds. Moreover, the model can disentangle background from dynamic objects, allowing us to create new games by swapping components as shown in the center and right images of the bottom row.</em></p>
</div>

<p>Building on the foundation laid by GameNGen, newer models such as DIAMOND (2024) extended the idea of diffusion-based world modeling while directly addressing some of these limitations. DIAMOND uses a more optimized sampling process based on the Elucidated Diffusion Models (also called “EDM”) framework, which reduces the number of denoising steps needed for generating a frame. This leads to significant speedups without sacrificing visual fidelity. More importantly, DIAMOND emphasized the importance of preserving visual detail for reinforcement learning agents: even small improvements in pixel accuracy translated to better decision-making in agents trained purely in simulation. It also introduced an action-conditioned training setup that enabled better controllability, allowing the model to generate scenes that actually respond to input actions. DIAMOND outperformed all previous approaches on the Atari 100K benchmark and even demonstrated impressive results on a real-world FPS game (CS:GO), showing that diffusion-based world models could scale beyond 2D arcade games. Together, these works reflect a major evolution in learned simulators—from basic frame predictors to highly detailed, interactive neural environments that blur the line between video generation and game engine functionality.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/student-19/figure2.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Figure 2. Images captured from people playing with keyboard and mouse inside DIAMOND’s diffusion world model. This model was trained on 87 hours of static Counter-Strike: Global Offensive (CS:GO) gameplay (Pearce and Zhu, 2022) to produce an interactive neural game engine for the popular in-game map, Dust II. Best viewed as videos at https://diamond-wm.github.io.</em></p>
</div>

<h2 id="comparison">Comparison</h2>
<h3 id="methodology">Methodology</h3>

<p>Since this report focuses on a literature-based review without running code or experiments, the comparison of models is conducted through a conceptual and technical analysis of their published papers, reported results, and architectural designs. Specifically, I analyze each model across the following key dimensions:</p>

<ul>
  <li><strong>Architecture</strong>: The type of model used (e.g., VAE-RNN, GAN, Transformer, Diffusion) and its general structure.</li>
  <li><strong>Visual Quality</strong>: The realism and coherence of the generated frames, often judged via FVD scores, human evaluation, or qualitative inspection.</li>
  <li><strong>Speed and Efficiency</strong>: The reported or estimated inference speed, often measured in FPS or number of frames generated per second.</li>
  <li><strong>Controllability</strong>: How well the model responds to agent actions and whether it supports meaningful interaction.</li>
  <li><strong>Generalization</strong>: The model’s ability to work across varied game environments or settings, beyond the specific domains it was trained on.</li>
  <li><strong>Use Case Fit</strong>: Whether the model is better suited for reinforcement learning simulation, visual imitation, real-time play, or offline rollout.</li>
</ul>

<p>I rely on each paper’s empirical results, qualitative demos, and architectural insights to draw these comparisons, aiming for a high-level yet grounded evaluation of where each model stands.</p>

<h3 id="comparison-table">Comparison Table</h3>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Architecture</th>
      <th>Visual Quality</th>
      <th>Speed / FPS</th>
      <th>Controllability</th>
      <th>Generalization</th>
      <th>Use Case Fit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>World Models</strong> [1]</td>
      <td>VAE + RNN + Controller</td>
      <td>Low (latent-only)</td>
      <td>Fast</td>
      <td>Weak (abstract input)</td>
      <td>Low (simple tasks)</td>
      <td>Latent planning, early imagination</td>
    </tr>
    <tr>
      <td><strong>GameGAN</strong> [2]</td>
      <td>GAN-based video model</td>
      <td>Moderate (GAN artifacts)</td>
      <td>Medium</td>
      <td>Weak–Moderate</td>
      <td>Moderate (2D games)</td>
      <td>Visual imitation, offline simulation</td>
    </tr>
    <tr>
      <td><strong>IRIS</strong> [3]</td>
      <td>VQ-VAE + Transformer</td>
      <td>Moderate (token-based)</td>
      <td>Fast</td>
      <td>Moderate</td>
      <td>Moderate (Atari)</td>
      <td>Efficient Atari world modeling</td>
    </tr>
    <tr>
      <td><strong>GameNGen</strong> [4]</td>
      <td>Diffusion (video)</td>
      <td>High (DOOM-level detail)</td>
      <td>Slow (~20 FPS)</td>
      <td>Weak–Moderate</td>
      <td>Limited (mostly DOOM)</td>
      <td>High-fidelity simulation</td>
    </tr>
    <tr>
      <td><strong>DIAMOND</strong> [5]</td>
      <td>Diffusion + EDM</td>
      <td>High (Atari + CS:GO)</td>
      <td>Medium–Slow</td>
      <td>Strong (action-guided)</td>
      <td>High (2D + 3D)</td>
      <td>RL rollout, controllable simulation</td>
    </tr>
    <tr>
      <td><strong>MineWorld</strong> [6]</td>
      <td>Tokenized Transformer</td>
      <td>Moderate–Good (MCraft)</td>
      <td>Fast</td>
      <td>Strong</td>
      <td>High (open-world envs)</td>
      <td>Real-time interaction, Minecraft AI</td>
    </tr>
  </tbody>
</table>

<h3 id="advantages-and-disadvantages">Advantages and Disadvantages</h3>
<p>World Models (Ha &amp; Schmidhuber, 2018) introduced a foundational framework that combined a variational autoencoder (VAE), a recurrent world model (RNN), and a simple controller to allow agents to learn entirely within a compressed simulation. The biggest strength of this approach was its simplicity and efficiency—it could represent complex visual environments like car racing in a compact latent space, enabling fast simulation and lightweight training. However, the trade-off was a loss in visual fidelity. Because the model learned to represent only abstract latent features rather than full-resolution frames, it often missed small but important visual cues. This made it less suitable for tasks where detailed spatial information or pixel-level feedback was crucial.</p>

<p>GameGAN (Kim et al., 2020) took a different approach by directly learning to generate game frames using GANs, conditioned on previous frames and player actions. This allowed the model to learn game mechanics visually, without being explicitly programmed. It was a breakthrough in terms of realism for simple arcade-style games like Pac-Man. The major advantage was its end-to-end frame generation, which made it more intuitive and visual than latent-based approaches. However, GameGAN faced several limitations. It struggled to generalize to more complex or 3D environments, produced visual artifacts under longer rollouts, and often failed to capture longer-term game dynamics, making it unreliable for extended agent interaction.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/student-19/figure3.png" style="width: 300px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Figure 3. Comparison from GameGAN: Box plot for Come-back-home metric. Lower is better. As a reference, a pair of randomly selected frames from the same episode gives a score of 1.17 ± 0.56.</em></p>
</div>

<p>IRIS (2022) improved upon earlier world modeling efforts by combining discrete latent tokenization (via VQ-VAE) with an autoregressive Transformer architecture to predict future observations. This method proved to be very sample-efficient, achieving strong performance on Atari with limited training data. Its key strengths were in planning and sequence modeling—the Transformer could reason over long horizons better than RNNs, and the tokenized representations kept the input compact and manageable. Still, IRIS inherited a core limitation from its discretization step: subtle visual details often got lost or blurred, which could affect tasks where pixel accuracy matters, such as tracking small objects or UI elements in games.</p>

<p>GameNGen (2024) represented a leap forward by introducing diffusion models into the world modeling space. Unlike earlier models that worked with compressed latent spaces or low-res visuals, GameNGen was trained to directly generate high-resolution game frames (from DOOM) using a video diffusion pipeline. This gave it a huge boost in visual quality—even human observers had difficulty distinguishing generated frames from real gameplay. It also incorporated techniques like noise-augmented training to improve rollouts and added decoder fine-tuning for in-game UI clarity. However, diffusion models are notoriously slow, and GameNGen was no exception. Despite achieving ~20 FPS on a TPU, its computational demands make it impractical for lightweight or real-time settings. It also lacked a robust memory mechanism, which sometimes caused inconsistencies over longer sequences.</p>

<p>DIAMOND (2024) followed GameNGen but aimed to make diffusion-based simulation more practical and controllable. By switching to EDM (Elucidated Diffusion Models), DIAMOND significantly reduced the number of denoising steps needed during frame generation, speeding up inference while maintaining high visual quality. It also emphasized the importance of preserving visual details for reinforcement learning, showing that better visuals could directly translate into better policy learning. DIAMOND incorporated action-conditioning more explicitly, making its predictions more responsive and meaningful. It achieved top performance on Atari benchmarks and even scaled to more complex environments like CS:GO. However, it still required substantial computational resources, and its real-time interactivity—while improved—was not yet on par with faster autoregressive models.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/student-19/figure4.png" style="width: 800px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Figure 4. Consecutive frames imagined with IRIS (left) and DIAMOND (right). The white boxes highlight inconsistencies between frames, which we see only arise in trajectories generated with IRIS. In Asterix (top row), an enemy (orange) becomes a reward (red) in the second frame, before reverting to an enemy in the third, and again to a reward in the fourth. In Breakout (middle row), the bricks and score are inconsistent between frames. In Road Runner (bottom row), the rewards (small blue dots on the road) are inconsistently rendered between frames. None of these inconsistencies occur with DIAMOND. In Breakout, the score is even reliably updated by +7 when a red brick is broken.</em></p>
</div>

<p>MineWorld (2025) tackled the problem from the opposite direction. Instead of focusing on high-fidelity image generation, it aimed for real-time, controllable simulation using an autoregressive Transformer trained on Minecraft gameplay. By tokenizing both visual and action data, MineWorld created a compact representation that could be processed quickly in parallel, achieving interactive speeds of several frames per second. This made it ideal for real-time agent interaction, especially in open-ended environments like Minecraft. The model was also open-source and included useful metrics to evaluate controllability and action alignment. However, its visual output was less detailed than diffusion models, and the heavy reliance on Minecraft-specific discretization raised questions about generalization to other environments or tasks with more complex visuals.</p>

<h2 id="discussion">Discussion</h2>
<p>Looking across the models I have reviewed, including World Models, GameGAN, IRIS, GameNGen, DIAMOND, and MineWorld, a few key patterns and trade-offs may be emerged. One of the most important is the balance between visual fidelity and computational efficiency. Models like GameNGen and DIAMOND prioritize generating highly detailed and realistic frames using diffusion processes. This level of realism is important for agent decision-making in environments where small visual cues can change outcomes, such as spotting an enemy or a power-up. However, these benefits come at a cost. Diffusion-based models require iterative sampling, which slows down generation significantly. Even with optimization tricks like the ones DIAMOND uses, they still lag behind autoregressive models in terms of speed and usability for real-time interaction.</p>

<p>On the other side of the trade-off are models like IRIS and MineWorld. These transformer-based architectures tokenize input data and process it sequentially or in parallel, allowing for much faster inference. Specifically, MineWorld is designed with real-time interaction in mind, decoding multiple spatial tokens in parallel to reach playable frame rates. This makes it far more suitable for live environments, such as interactive simulations or multi-agent games. However, the drawback is that these tokenized models often lack the richness of full-frame diffusion outputs. They abstract away some details at the level of pixel, which might be acceptable for planning but could be a limitation in tasks requiring high visual precision.</p>

<p>Another dimension of comparison is controllability—how well a model can respond to specific player or agent actions. GameGAN and early latent-based models tended to blur this relationship, generating plausible but sometimes unrelated frames. In contrast, DIAMOND and MineWorld both place a strong emphasis on action-conditioning, ensuring that generated outcomes align more closely with inputs. This is especially important for reinforcement learning or agent-based tasks where the quality of feedback directly affects learning. Action-aware training objectives, like the ones used in DIAMOND, have proven to significantly improve this aspect.</p>

<p>I also noticed differences in how well these models generalize to more complex or diverse environments. Earlier models were usually tested on simple settings like Atari or 2D arcade games. GameNGen moved to DOOM, a more dynamic and visually rich environment, while MineWorld tackled the challenge of Minecraft, which is both open-ended and visually complex. This progression reflects growing confidence in these architectures, but also highlights how scaling up introduces new challenges—especially around long-term memory, scene coherence, and temporal consistency.</p>

<p>Overall, there’s no single model that “wins” in every category. Each represents a different point along the spectrum of speed, quality, and control. For applications like agent training in offline simulators, slower but richer models like DIAMOND may be more useful. For interactive settings or games, MineWorld’s transformer-based design is a better fit. These trade-offs are likely to remain central as the field continues to explore what the ideal neural game engine should look like.</p>

<h3 id="conclusion-and-future-work">Conclusion and Future Work</h3>
<p>In this report, I examined the evolution of learned world models and neural game engines, starting from early latent-based simulators like World Models and IRIS, to more visually sophisticated generators like GameNGen and DIAMOND, and finally to real-time transformer-based systems like MineWorld. Each model reflects a different design philosophy and set of trade-offs between visual quality, generation speed, controllability, and generalization. GameNGen and DIAMOND showed how diffusion models can deliver high-fidelity visuals and boost agent performance, but they remain limited by their computational demands and relatively slow inference. On the other hand, models like MineWorld prioritize usability and speed, making them more practical for interactive or online applications, even if they compromise on pixel-level detail.</p>

<p>Across all these models, one theme stands out: the growing ambition to build neural simulators that are not just generative, but interactive, responsive, and reliable. Whether the goal is to train reinforcement learning agents, generate synthetic training data, or build immersive game environments, the requirements go far beyond simple frame prediction. We now need models that can reason over time, respond meaningfully to input actions, and scale to diverse, dynamic worlds. While current architectures have taken big steps in this direction, challenges like long-term memory, cross-domain generalization, and efficient conditioning still remain open problems.</p>

<p>Looking ahead, promising directions include hybrid models that combine the realism of diffusion with the speed of token-based transformers, memory-augmented agents that can retain state over long episodes, and modular systems that separate world dynamics from rendering or control. Another key area is improving action controllability without sacrificing sample efficiency—potentially using auxiliary objectives, disentangled representations, or contrastive methods. As this field matures, we expect the boundary between learned models and traditional engines to blur even further, opening up exciting opportunities not just in AI, but in game design, robotics, and interactive media more broadly.</p>

<h2 id="references">References</h2>
<p>[1] D. Ha and J. Schmidhuber, World Models, arXiv:1803.10122, 2018. Available: https://arxiv.org/abs/1803.10122</p>

<p>[2] S. Kim, Y. Choi, J. Yu, J. Kim, J. Ha, and B. Zhang, Learning to Simulate Dynamic Environments with GameGAN, arXiv:2005.12126, 2020. Available: https://arxiv.org/abs/2005.12126</p>

<p>[3] V. Micheli, E. Grefenstette, and S. Racanière, Transformers are Sample-Efficient World Models, arXiv:2209.00588, 2022. Available: https://arxiv.org/abs/2209.00588</p>

<p>[4] Y. Valevski, O. Sharir, A. Gordon, M. Tal, A. Bar, A. Azaria, N. Shenfeld, Y. Meshulam, J. Berant, and S. Shalev, Diffusion Models Are Real-Time Game Engines, arXiv:2408.14837, 2024. Available: https://arxiv.org/abs/2408.14837</p>

<p>[5] M. Alonso, A. Ramesh, Y. B. Kim, M. G. Azar, M. Janner, P. Agrawal, Y. Chebotar, and Y. Tassa, Diffusion for World Modeling: Visual Details Matter in Atari, arXiv:2405.12399, 2024. Available: https://arxiv.org/abs/2405.12399</p>

<p>[6] J. Fang, Y. Wang, W. Shao, Y. Zhao, Z. Wang, M. Yang, W. Zhan, B. Dai, H. Shi, C. Liu, B. Zhou, and J. Wang, MineWorld: A Real-Time and Open-Source Interactive World Model on Minecraft, arXiv:2505.14357, 2025. Available: https://arxiv.org/abs/2505.14357</p>

<hr />]]></content><author><name>Yi Han</name></author><summary type="html"><![CDATA[This report explores recent advances in neural game engines, with a focus on generative models that simulate interactive game environments. Based on the prior presentation of GameNGen, a diffusion-based video model trained on DOOM, I review and compare several state-of-the-art approaches including DIAMOND, MineWorld, IRIS, GameGAN, and the original World Models. I also analyze their differences in architecture, visual fidelity, speed, and controllability, highlighting the trade-offs each design makes. Finally I conclude with a discussion on future directions for building responsive, efficient, and generalizable neural simulators for reinforcement learning and interactive media.]]></summary></entry><entry><title type="html">Peek-A-Boo, Occlusion-Aware Visual Perception through Active Exploration</title><link href="http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo.html" rel="alternate" type="text/html" title="Peek-A-Boo, Occlusion-Aware Visual Perception through Active Exploration" /><published>2024-12-13T00:00:00-08:00</published><updated>2024-12-13T00:00:00-08:00</updated><id>http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo</id><content type="html" xml:base="http://localhost:4000/CS269-Projects-2025Spring/2024/12/13/student-01-peekaboo.html"><![CDATA[<blockquote>
  <p>In this study, we present a framework for enabling
robots to locate and focus on objects that are partially or fully
occluded within their environment. We split up any robotic tasks
into two steps: Localization, where the robot searches for objects
of interest, and Task Completion, where the robot completes the
task after finding the object. We propose Peekaboo, a solution
to the Localization stage to find partially or even fully occluded
objects. We train a reinforcement learning algorithm to teach
the robot to actively reposition its camera to optimize visibility
of occluded objects. The key features include engineering a
reward function that incentivizes effective object localization and
setting up a comprehensive training environment. We develop
a simulation environment with randomness to learn to localize
from numerous initial viewpoints. Our approach also includes
the implementation of a vision encoder for processing visual
input, which allows the robot to interpret and respond to
objects and occlusions. We design metrics to quantify the model’s
performance, demonstrating its capability to handle occlusions
without any human intervention at all. The results of this work
showcase the potential for robotic systems to actively improve
their perception in cluttered or obstructed environments.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#prior-works" id="markdown-toc-prior-works">Prior Works</a></li>
  <li><a href="#problem-formulation" id="markdown-toc-problem-formulation">Problem Formulation</a></li>
  <li><a href="#proposed-methods-and-evaluation" id="markdown-toc-proposed-methods-and-evaluation">Proposed Methods and Evaluation</a></li>
  <li><a href="#experiments" id="markdown-toc-experiments">Experiments</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
  <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ul>
      <li><a href="#future-work" id="markdown-toc-future-work">Future Work</a></li>
    </ul>
  </li>
  <li><a href="#links" id="markdown-toc-links">Links</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In dynamic and cluttered environments, robotic systems
with visual perception capabilities must be able to adapt their
viewpoints to maintain visibility of target objects, even when
occlusions or obstacles obstruct their line of sight. Traditional
robotic vision systems often address this requirement by either
using a large array of static sensors, usually cameras, pointing in different orientations, or by moving a single
sensor in a predetermined path. Both approaches have their
downsides. Arrays of static sensors are far more expensive than
using a single sensor, and rely on the motion of their agent
to acquire new viewpoints. Sensors that follow predetermined
paths lack the flexibility to capture environment-dependent
information about complex scenes common in real-world
settings. These limitations serve as critical bottlenecks slowing
the advancement of vision-based robotics. Active Vision is a
promising field that aims to address these challenges. The goal
of Active Vision is to mimic the way that humans perceive
their environment: by learning to move and orient a single
sensor in ways that capture information important for completing some task.</p>

<p>Active Vision avoids the shortcomings of
the other two approaches; the agent uses a single controllable
sensor, as opposed to an array of fixed sensors to perceive
its environment, and the agent can learn to manipulate its
sensor in response to its environment in nuanced ways that
a predetermined approach could not.</p>

<p>Recent advances in reinforcement learning (RL) have looked
into enabling robots to learn behaviors directly from their
interactions with the environment, making it possible to train
autonomous systems to explore the environment, allowing
for active perception. In robotic vision tasks, RL algorithms
can enable a camera mounted on a robotic arm to not only
locate objects but also to continuously adjust its position
to avoid occlusions and improve object visibility. However,
developing such an RL framework requires overcoming several
challenges, including creating a robust training environment
that mimics the need to shift camera perspectives, and engineering reward functions that incentivize behavior promoting
consistent visibility.</p>

<p>We propose Peekaboo, an approach that addresses many
of the shortcomings of previous methods. Peekaboo’s key
insight is that the Localization and Task Completion steps in
manipulation tasks can be decoupled with minimal loss of
generality. In practice, when humans perform manipulation
tasks, we search our environment for an object before extending a hand in its direction to grasp it. The same logic applies here. Any manipulation task first requires the agent
to localize the object, along with anything else critical for
completing the task. Peekaboo focuses on this Localization
step, but unlike previous works is designed to be robust to
very heavy occlusions, and allows the camera to be controlled
with many degrees of freedom.</p>

<h2 id="prior-works">Prior Works</h2>

<p>Prior works in Active Vision address similar problems to
ours, but all differ in some key areas. While there are many
works in Active Vision, we mention those most relevant to
our method. A recent approach by researchers at Google
Deepmind proposes using navigation data to enhance the
performance of cross-embodiment manipulation tasks [8]. The
researchers demonstrate that learning a general approach to
navigation from sources using different embodiments is key
to performing manipulation tasks. Their findings reinforce the
notion that navigation is a robotics control primitive that can
aid any task, even those that do not explicitly perform navigation.</p>

<p>Other works like those by researchers from the University of Texas at Austin have proposed performing complex
tasks using an “information-seeking” and an “information-receiving” policy to guide the search task and manipulation task separately [3]. While impressive, their approach Learning
to Look is designed to perform complex manipulation tasks in
simple unobstructed environments with limited camera motion.
In contrast, our goal is to operate in environments designed to
hinder search tasks and force the agent to substantially move
its camera to succeed. There has also been plenty of work
in using Active Vision as a tool to improve performance in
well-studied tasks like grasping [6] and object classification
[7]. More recently, there has been an increased focus on
learning Active Vision policies from human demonstrations
using Imitation Learning. Ian Chuang et al. propose a method
that allows a human teleoperator to control both a camera and
a robotic arm in a VR environment in order to collect near-optimal human demonstration data of Active Vision tasks like inserting a key into a lock.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/pworksfig1.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Using learned Active Vision policies as a method to acquire novel views of objects to aid in object classification.</em></p>
</div>

<p>Two prior works stand out as particularly similar to our task
and deserve additional attention. The first approach by Stanford researchers, DREAM, proposes decoupling their agent’s
task into an explorative navigation step and exploitative task
completion stage, each of which is learned separately [5]. This
approach relies on an intrinsic task ID, and leverages memory
from previous explorative iterations through a memorization
process to aid it in the current iteration. While the researchers
demonstrate impressive results, we find that their environment
is somewhat limited. Since the researchers are performing
navigation, their agent is limited to moving in two degrees
of freedom, and rotating in a single degree of freedom. In
contrast, our approach freely manipulates the agent in six
degrees of freedom. In addition, their task is designed such
that the agent can physically access the entire environment:
an assumption that is often not true in physical systems in the
real world.</p>

<p>Another highly relevant paper from researchers
at Carnegie Mellon uses Active Vision to aid a manipulation
task in the presence of visual occlusions [1]. The authors
propose a task of pushing a target cube onto a target coordinate
using a robotic arm, where the agent learns to independently
manipulate its robotic arm and the camera from which it sees
to avoid physical occlusions preventing task completion. Like
DREAM, the proposed method limits the agent to few degrees
of freedom in its motion. In addition, the agent operates on
fairly “weak” occlusions that rarely impede task completion.
Lastly, the authors learn a single policy that jointly manipulates
the robotic arm and controls the camera – a process we
believe can be decoupled.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/pworks2.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Learning to perform manipulation in the presence of visual occlusions by learning to control the camera. The agent jointly learns to control the robotic arm to move the cube onto the target, and control the camera to avoid visual obstructions.</em></p>
</div>

<h2 id="problem-formulation">Problem Formulation</h2>

<p>We contribute a novel task formulation unique to Peekaboo,
and central to performing occlusion-aware search. We train
Peekaboo using a Reinforcement Learning framework, which
requires us to define three components: the environment, the
agent, and the reward function.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/mdp.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Markov Decision Process (MDP) Framework</em></p>
</div>

<p>The environment is a simple indoor room, containing a
central table. On the table lies a randomly positioned target
cube and a large randomly positioned wall meant to block the
view of the cube.</p>

<p>The agent is a 6-DoF Panda robotic arm, initialized to look
in a random direction. It can move its end effector in any
direction, and rotate its end effector to any orientation. The
agent observes its environment through a sensor in its hand
that captures images from the hand’s point of view.
The reward function is a binary reward function. The value
of the reward is one if the target cube is within the frame of
the robotic arm’s camera observation. Otherwise, if the cube
is off-frame, or occluded by the wall, the value of the reward
is zero. This reward function encourages the agent to search
its environment until it can see the target cube.
Using this particular task, we aim to train Peekaboo such
that it can learn to search around occlusions to locate the target
cube, regardless of variations in the scene.
In summary, we focus on building a flexible, RL-based
framework that enables a robotic arm-mounted camera to
autonomously determine the optimal perspectives for tracking
objects, particularly in environments where occlusions are
frequent. To achieve this, we will:</p>

<ol>
  <li><strong>Introduce randomness</strong>
    <ul>
      <li>Randomize both the environment and camera settings during the training phase.</li>
      <li>Ensure the model is robust to variable conditions.</li>
    </ul>
  </li>
  <li><strong>Include a vision encoder</strong>
    <ul>
      <li>Process incoming visual data effectively.</li>
    </ul>
  </li>
  <li><strong>Incorporate a reinforcement learning (RL) algorithm</strong>
    <ul>
      <li>Train the camera’s decision-making process.</li>
    </ul>
  </li>
  <li><strong>Engineer a reward function</strong>
    <ul>
      <li>Emphasize maintaining clear sightlines.</li>
      <li>Penalize occlusions to improve performance.</li>
    </ul>
  </li>
</ol>

<p>This work contributes to the growing field of adaptive
robotic vision, offering a method for giving robots autonomous, context-aware vision capabilities that can support
applications requiring real-time adaptability in unpredictable
environments.</p>

<h2 id="proposed-methods-and-evaluation">Proposed Methods and Evaluation</h2>
<p>We propose a two stage RL training framework that allows
our robot to first search for and focus on the object of interest,
and then complete the desired task by interacting with the
object. The focus of our methods will be on training and
testing the first stage of this framework, as there are many
prior works that focus on task completion. We will refer to
this first stage as Localization and the second stage as Task
Completion. The Localization phase will consist of training
an RL agent that learns to move an egocentric, or first person
view, camera to search around occlusions by exploring the
environment, with the end goal of localizing the object of
interest in its frame of view.</p>

<p>We will approach this problem using a custom environment
in Robosuite. Our environment will be built off of the base
Lift task environment, with modifications made to create
occlusions. Specifically, we will put a wall in between the
robotic arm and the cube that the arm is trying to lift. This
will be the main occlusion that is dealt with in our Wall task.
In its initial frame observation of the environment, the robot
will not be able to see the cube it is trying to lift, as it will be
covered by the wall. We also introduced cube randomization,
camera randomization, and arm randomization. The cube and
arm randomization was done through the Robosuite framework. The camera and wall randomization relied on custom
quaternions that changed the orientation of our objects. The
goal of the overall task is for the robot to search for the cube,
find it behind the wall, and then lift it.</p>

<p>Through our two stage framework, we propose a MDP
formulation for the Localization stage. The state space
consists of the image observations from our robot. We will
have a camera attached the end effector of the robotic arm
to observe these images. The robot will not have access to
ground truth proprioceptive data. The action space consists
of modifying the perspective of the camera, which is done
through the Cartesian coordinates of moving the robotic arm.
The reward function will be as follows:</p>

\[\text{reward} = \begin{cases} 
      0 &amp; \text{if not in view} \\
      1 &amp; \text{if in view} 
   \end{cases}\]

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig1.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 1. Visualization of our Reward Function</em>.</p>
</div>

<p>If the cube is in view, we get a reward of 1, and if it is not, the reward is 0. We will implement this in the environment using the ground truth proprioceptive data of the robotic arm, the wall, and the cube. Using this position data, we can calculate the angle in between the robotic arm and wall, and the robotic arm and the cube. These angles can then indicate if the wall is blocking the robotic arm’s frame of view from the cube or not, which allows us to return a reward of 0 or 1. Given that the robot does not have access to this ground truth data, we are essentially teaching the robot to recognize and search around occlusions when they show up prominently in its camera frame.</p>

<p>Here is a snippet from our code:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def reward(obs):
    target_vertices, wall_vertices, camera_position, camera_vec, camera_bloom = preprocess(obs)

    # if any corner of the target cube is outside of frame, return 0
    for target_vertex in target_vertices:
        if not target_visible_in_conical_bloom(target_vertex, camera_position, camera_vec, camera_bloom):
            return 0

    # if any corner of the target cube is blocked by the wall, return 0
    for target_vertex in target_vertices:
        for wall_plane in wall_vertices:
            if line_segment_intersects_truncated_plane(target_vertex, camera_position, wall_plane):
                return 0
        
    # cube is entirely within bloom and is not obstructed
    return 1
</code></pre></div></div>

<p>Once we have set up the Robosuite environment with the camera and reward function, we will train a PPO agent on the MDP formulation of the \(\textit{Localization}\) stage. The image observations will be processed by a pretrained Vision Encoder, which will be frozen during training. The output features of the Vision Encoder will be the inputs to our RL neural networks. Our evaluation will consist of visualizing rollouts and establishing a success rate for the \(\textit{Localization}\) phase. A rollout will be considered a success if the object of interest is seen completely unobstructed in the scene from the perspective of the robot. In order to test for generalization, we will randomize initial states of the robotic arm, the wall, and the cube. Sometimes, the cube will be completely in view, and sometimes it will be completely occluded. It will be up to the robot to learn and decide when it needs to search around the wall and when it has the object of interest in sight.</p>

<p>Our main contributions are the training and evaluation of this first stage. Now that we have run randomized tests, we have set up a foundation to implement the second stage of our proposed method. We can simply use the final, localized state of our first stage as the initial state for our \(\textit{Task Completion}\) stage, which will also be trained using the PPO algorithm. The goal then would be to first use the \(\textit{Localization}\) stage to find the desired object behind any occlusions, and then use the \(\textit{Task Completion}\) stage to execute the task. We can then visualize rollouts of this end to end pipeline and determine the success rate.</p>

<p>This two stage training can then be compared against a baseline of training one PPO agent for the entire task from scratch. In effect we want to show that while one agent is not able to both find the object and finish the task in one go, our two step approach is able to successfully break this down into multiple steps.</p>

<h2 id="experiments">Experiments</h2>
<p>Our experimental environment is in Robosuite. We use a custom Robosuite environment, built off their Lift Environment implementation. This environment provides a target cube object, a robotic arm, a mounted camera on the robotic arm, and a table. We customize this class by adding randomness to the initial positions and orientations of all of the above objects except the table. We also add our own wall, randomized in size, positioning, and orientation. Fig. 2 shows the top down view of our experiment setup. This figure is for visual understanding purposes only and the images from this camera are not used in our training. Fig. 3 shows the first person images from our mounted camera, randomized at each initialization. We use images from this camera to train the arm to move the camera to place the target object in frame. For our training runs, we can train models with varying degrees of randomization. Specifically, in this project, we experimented with turning camera randomization on and off. For all of our training runs, we kept wall and cube randomization always on.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig2.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 2. Examples of Top down view of randomized initialization</em>.</p>
</div>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig3.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 3. Examples of First person view of randomized initialization</em>.</p>
</div>

<p>For our training process, we interpret the camera image using DINOv2, a vision transformer based vision encoder, and we feed the results into our reward function. This vision encoder takes in images of size 224 by 224 from the environment and outputs 384 features to our RL policy.</p>

<p>We use a proximal policy optimization (PPO) reinforcement learning algorithm that trains our model to look at the target object. We initialize our RL policy to be a 2 layer MLP with 256 activations per layer, taking the feature inputs from our vision encoder and outputing the action for our robotic arm. We configure this model through the Stable-Baselines3 library. Ideally, we would like to train our policy for 1 million timesteps, which amounts to 2000 episodes, but due to training time constraints, we begin with 200k timesteps, which amounts to 400 episodes. Further discussion of training is presented in our results.</p>

<p>Our metrics for the model that we trained are the rewards that we defined earlier. We set the episode horizon to 500, meaning that there is a total possible reward per episode of 500. A lower reward indicates that the policy struggled to find the cube. The closer the reward is to 500 the better, indicating that our policy was able to localize the cube early in the episode, and learned to keep it within view the entire time.</p>

<h2 id="results">Results</h2>

<p>For our results, we evaluate two main training runs to demonstrate the performance of our method. As we are working with a completely custom environment and task, we are evaluating the performance of an approach to a new problem formulation.</p>

<p>Initially, we trained a model on our fully randomized environment, including randomization of the cube, wall, and camera angle. The reward graph for the training of this model is presented in Figure 4. The model is trained for 200k timesteps, which is about 400 episodes, and took approximately 10 hours to train. We can see that the general trend of the rewards is upwards, with an increase in rewards at around the 130k timesteps mark. However, the rewards are also oscillating after that point, showing variability and lack of convergence to our results.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig4.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 4. Rewards Graph for training with randomization of wall, cube and
camera</em>.</p>
</div>

<p>We wanted to demonstrate more stability within our results, so the next model we trained was without camera randomization. The reward graph for this training run is presented in Figure 5. We see an increase in rewards earlier in this graph at 90k timesteps, but then it drops again before picking up towards the end of training. Again, we see that our rewards do not converge.</p>

<div style="text-align: center;">
  <img src="/CS269-Projects-2025Spring/assets/images/group-01/fig5.png" style="width: 500px; max-width: 100%;" alt="RobosuiteEnv" />
  <p><em>Fig 5. Rewards Graph for training with randomization of only wall and cube</em>.</p>
</div>

<p>In order to test if the model converges, we decided to continue training the model with full environment randomization (camera, wall, and cube) for 1 million time steps, which took 40+ hours. Unfortunately, this training run did not converge either, and showed the same up and down performance that our shorter training runs displayed. Therefore, we see that naively training the model for longer does not improve performance.</p>

<h2 id="discussion">Discussion</h2>
<p>When we tested our model after training, we saw that our policy’s actions greatly depends on the initial conditions of the task. Since we are randomizing the environment before every episode, the initial conditions could vary from episode to episode. In the case that the wall or cube are in view in the beginning, the model performs reasonably by searching around them. However, we also see many cases when the robotic arm is pointed in an arbitrary direction away from the wall and cube. In this case, it sees just a blank white image of the table. This initial observation gives the robot no information with which to find the cube or even start searching, since that image is the only observation (we do not give the robot proprioceptive data). There is no indication within the image which way it should even search, as that would change from episode to episode due to environment randomization. We believe that this variance in the task causes instability in performance, which aligns with the results we are seeing. When there is a favorable initial condition, the model learns well, but when there is no useful information in the initial condition, performance declines and causes instability in  rewards. This performance would explain why our training graphs look the way they do, and why training does not converge simply with training for longer.</p>

<p>It is also important to note that even though we trained for 10+ hours, that only amounted to 400 episodes, which is not very much compared to other environments that train RL models with training episodes in the thousands. This increase in compute goes back to our problem setup, where we settled on the computationally expensive DinoV2 vision encoder with high resolution images, and set the complexity of our task through drastic randomization of our task environment. In future works, especially with a visually simple task environment (the only objects are a wall and cube, with no visual intricacies) it may have been better to use a smaller Vision Encoder like a ResNet or train our own CNNPolicy with lower resolution images. Additionally, we had hoped that the exploration of RL would allow it to learn to overcome drastic randomization, but through our results it seems that we were potentially wrong. It may have been better to start with no randomization and just a wall occluding a cube, and once that was working, we could incrementally add randomization to increase complexity. Once these changes are implemented, we believe that our solution could be a viable solution to the active exploration problem that we are working on.</p>

<h2 id="conclusion">Conclusion</h2>
<p>We present Peek-A-Boo, a two stage Reinforcement Learning framework that aims to break down any task into two stages: Localization/Search and Task Completion. We create a custom task environment with occlusions and randomization, an engineered reward function, and train an RL model with a Visual Encoder to complete the Localization stage. Through our model training, we are able to demonstrate promise in our approach through generally increasing rewards despite variability in training performance.</p>

<h3 id="future-work">Future Work</h3>
<p>Future work would focus on simplifying the task with less drastic randomization to show stable performance. Then, we can scale up to more complicated tasks. Future work could also look into other ways to stabilize the performance of the model. For instance, the Localization stage of our framework could be approached using IBRL (Imitation Bootstrapped Reinforcement Learning) [4] in order to improve stability through Imitation Learning while allowing the model to also explore the environment through RL. Additionally, another avenue of future work could be automating the reward function using object detection or semantic segmentation on the observation image. We have engineering a reward function using ground truth data, but that may not always be available when applying this framework.</p>

<p>Overall, we see this as a promising first step for active perception and occlusion-aware robotics. We are excited to see the advancements in this field to hopefully one day see robot policies that are able to explore and generalize to any extenuating circumstance that they face in their environment.</p>

<h2 id="links">Links</h2>
<p><a href="https://github.com/ophirsim/Peekaboo">Link to our codebase</a></p>

<p>Custom feature extractor using the pretrained DINOv2 base model from timm: <a href="https://github.com/ophirsim/Peekaboo/blob/main/vision_encoder.py">Link</a></p>

<p>Stable Baselines Documentation: <a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">Link</a></p>

<h2 id="references">References</h2>
<p>[1] Cheng, R., Agarwal, A., &amp; Fragkiadaki, K.
(2018). Reinforcement Learning of Active Vision
for Manipulating Objects under Occlusions.
Conference on Robot Learning, 422-431.
http://proceedings.mlr.press/v87/cheng18a/cheng18a.pdf</p>

<p>[2] Chuang, I., Lee, A., Gao, D., &amp; Soltani, I. (2024). Active
Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation. arXiv preprint
arXiv:2409.17435.</p>

<p>[3] Dass, S., Hu, J., Abbatematteo, B., Stone, P., &amp; Martin, R. (2024). Learning to Look: Seeking Information for Decision Making via Policy Factorization. arXiv
preprint arXiv:2410.18964.</p>

<p>[4] Hu, H., Mirchandani, S., &amp; Sadigh, D. (2024). Imitation
Bootstrapped Reinforcement Learning. arXiv [Cs.LG].
Retrieved from http://arxiv.org/abs/2311.02198</p>

<p>[5] Liu, E. Z., Finn, C., Liang, P., &amp; Raghunathan, A.
(2021, November 12). Decoupling exploration and exploitation in meta-reinforcement learning without sacrifices. Exploration in Meta-Reinforcement Learning.
https://ezliu.github.io/dream/</p>

<p>[6] Natarajan, S., Brown, G., &amp; Calli, B. (2021).
Aiding Grasp Synthesis for Novel Objects Using Heuristic-Based and Data-Driven Active Vision Methods. Frontiers in Robotics and AI, 8.
https://doi.org/10.3389/frobt.2021.696587</p>

<p>[7] Safronov, E., Piga, N., Colledanchise, M., &amp;
Natale, L. (2021, August 2). Active perception
for ambiguous objects classification. arXiv.
https://arxiv.org/pdf/2108.00737.pdf</p>

<p>[8] Yang, J., Glossop, C., Bhorkar, A., Shah, D., Vuong,
Q., Finn, C., … &amp; Levine, S. (2024). Pushing the
limits of cross-embodiment learning for manipulation
and navigation. arXiv preprint arXiv:2402.19432.</p>

<hr />]]></content><author><name>Medha Kini, Ophir Siman-Tov</name></author><summary type="html"><![CDATA[In this study, we present a framework for enabling robots to locate and focus on objects that are partially or fully occluded within their environment. We split up any robotic tasks into two steps: Localization, where the robot searches for objects of interest, and Task Completion, where the robot completes the task after finding the object. We propose Peekaboo, a solution to the Localization stage to find partially or even fully occluded objects. We train a reinforcement learning algorithm to teach the robot to actively reposition its camera to optimize visibility of occluded objects. The key features include engineering a reward function that incentivizes effective object localization and setting up a comprehensive training environment. We develop a simulation environment with randomness to learn to localize from numerous initial viewpoints. Our approach also includes the implementation of a vision encoder for processing visual input, which allows the robot to interpret and respond to objects and occlusions. We design metrics to quantify the model’s performance, demonstrating its capability to handle occlusions without any human intervention at all. The results of this work showcase the potential for robotic systems to actively improve their perception in cluttered or obstructed environments.]]></summary></entry></feed>